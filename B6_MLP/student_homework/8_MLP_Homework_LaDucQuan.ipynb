{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# I. Lý thuyết (10 câu, 0.5 điểm/câu)"
      ],
      "metadata": {
        "id": "SR4mkBllVIIw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Phát biểu nào sau đây là đúng về kiến trúc MLP\n",
        "\n",
        "A. Là kiến trúc gồm nhiều Layers liên tiếp xử lý dữ liệu theo thứ tự.\n",
        "\n",
        "B. Mỗi một layer của MLP sẽ bao gồm nhiều units, mỗi unit đóng vai trò tương tự như một biến.\n",
        "\n",
        "C. Thứ tự các layers của MLP là `input layer --> các hidden layers --> output layer`\n",
        "\n",
        "D. Cả ba đáp án trên."
      ],
      "metadata": {
        "id": "0nTCKfnmVP_6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> Chọn đáp án D"
      ],
      "metadata": {
        "id": "L0XTVFNoDCCB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Các đặc trưng được tạo ra từ MLP có gì khác biệt so với các thuật toán machine learning truyền thống ?\n",
        "\n",
        "A. Là những đặc trưng được xác định từ trước thông qua feature engineering từ người xây dựng mô hình.\n",
        "\n",
        "B. Là những đặc trưng được tạo thành từ Polynormial Feature.\n",
        "\n",
        "C. Là những đặc trưng đã được chuẩn hóa theo Min-max scaling.\n",
        "\n",
        "D. Là những đặc trưng ẩn (_latent features_) được học thông qua quá trình huấn luyện mô hình."
      ],
      "metadata": {
        "id": "A42FkbHNWC6J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> Chọn đáp án D"
      ],
      "metadata": {
        "id": "6GzUxo-PDKtm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Vì sao khi tính toán một unit tại layer tiếp theo thì cần áp dụng activation function lên tổ hợp tuyến tính của các units đầu vào từ layer ngay liền trước?\n",
        "\n",
        "A. Để giảm thiểu chi phí tính toán.\n",
        "\n",
        "B. Để khắc phục hiện tượng overfitting.\n",
        "\n",
        "C. Để biến đổi không gian features từ dạng tuyến tính sang dạng phi tuyến.\n",
        "\n",
        "D. Để khắc phục hiện tượng underfitting."
      ],
      "metadata": {
        "id": "AxRBoeqtWBky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> Chọn đáp án C"
      ],
      "metadata": {
        "id": "NeC8HiEiDQum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) Nếu mạng neural không áp dụng activation function thì điều gì sẽ xảy ra?\n",
        "\n",
        "A. Các units tại các hidden layers chỉ là biểu diễn tuyến tính của các biến đầu vào.\n",
        "\n",
        "B. Mô hình sẽ có chi phí tính toán cao.\n",
        "\n",
        "C. Cần rất nhiều hidden layers để mô hình có độ chính xác cao.\n",
        "\n",
        "D. Không thể huấn luyện được mô hình."
      ],
      "metadata": {
        "id": "JE2iRKHqXtRn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> Chọn đáp án A"
      ],
      "metadata": {
        "id": "qoHbJUv3DX_0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5) Quá trình Feed forward là gì ?\n",
        "\n",
        "A. Tính toán loss function.\n",
        "\n",
        "B. Tính toán đầu ra thông qua dựa vào đầu vào và các kết nối của mạng.\n",
        "\n",
        "C. Tính gradient descent.\n",
        "\n",
        "D. Đếm số lượng tham số của mô hình."
      ],
      "metadata": {
        "id": "WMNPhiPZYmnv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> Chọn đáp án B"
      ],
      "metadata": {
        "id": "vKSeU_QRDgfz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6) Mục tiêu của quá trình Back propagation là gì ?\n",
        "\n",
        "A. Tính output của mô hình.\n",
        "\n",
        "B. Tính gradient descent tại từng layers.\n",
        "\n",
        "C. Cập nhật trọng số của mô hình thông qua đạo hàm bậc nhất theo chiều ngược lại từ layer output trở về layer input.\n",
        "\n",
        "D. Tính loss function."
      ],
      "metadata": {
        "id": "2Nv6DGTKZKLL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> Chọn đáp án C"
      ],
      "metadata": {
        "id": "deLBuy0VDmqg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7) Làm sao để kiểm soát hiện tượng overfitting trong một mạng MLP?\n",
        "\n",
        "A. Thêm thành phần điều chuẩn là các norm chuẩn của trọng số vào loss function.\n",
        "\n",
        "B. Sử dụng dropout để loại bỏ các kết nối của mạng neural một cách ngẫu nhiên.\n",
        "\n",
        "C. Gia tăng số lượng layers.\n",
        "\n",
        "D. A và B\n"
      ],
      "metadata": {
        "id": "ZZixC86gZrIY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> Chọn đáp án A"
      ],
      "metadata": {
        "id": "UlTuFoaob-Yi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8) Để tính được phân phối xác suất tại đầu ra của bài toán phân loại đa biến của mạng MLP thì chúng ta sẽ?\n",
        "\n",
        "A. Sử dụng hàm softmax lên các units tại layers cuối cùng.\n",
        "\n",
        "B. Sử dụng hàm ReLU lên các units tại layers cuối cùng.\n",
        "\n",
        "C. Sử dụng hàm tanh lên các units tại layers cuối cùng.\n",
        "\n",
        "D. chỉ cần tổ hợp tuyến tính các units lại layers trước layers cuối cùng.\n",
        "\n"
      ],
      "metadata": {
        "id": "Y8ak0nKwaNbI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> Chọn đáp án A"
      ],
      "metadata": {
        "id": "iUBCqjOncBv7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9) Hàm loss function của mạng MLP có dạng như thế nào ?\n",
        "\n",
        "A. Là tổng Cross Entropy của toàn bộ các quan sát.\n",
        "\n",
        "B. Là tổng Cross Entropy của toàn bộ các nhãn.\n",
        "\n",
        "C. Là tổng Cross Entropy của toàn bộ các quan sát và toàn bộ các nhãn.\n",
        "\n",
        "D. A. Là tổng MSE của toàn bộ các quan sát."
      ],
      "metadata": {
        "id": "2H88XI9UatBL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> Chọn đáp án C"
      ],
      "metadata": {
        "id": "2fIQg2NmcOkz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10) Giả sử sau quá trình Back propagation tính được đạo hàm tại từng trọng số là $D_{ij}^{(l)}$ (là đạo hàm tương ứng với tham số $\\Theta_{ij}^{(l)}$ kết nối unit thứ $i$ của layer $(l-1)$ với unit thứ $j$ của layer $l$). $\\alpha$ là learning rate có giá trị dương và rất nhỏ. Trọng số sẽ được cập nhật như thế nào?\n",
        "\n",
        "A. $\\Theta_{ij}^{(l)} := \\Theta_{ij}^{(l)} + \\alpha D_{ij}^{(l)}$\n",
        "\n",
        "B. $\\Theta_{ij}^{(l)} := \\Theta_{ij}^{(l)} - \\alpha D_{ij}^{(l)}$\n",
        "\n",
        "C. $\\Theta_{ij}^{(l)} := \\Theta_{ij}^{(l)} - \\alpha ||D_{ij}^{(l)}||_1$\n",
        "\n",
        "D. $\\Theta_{ij}^{(l)} := \\Theta_{ij}^{(l)} - \\alpha ||D_{ij}^{(l)}||_2$"
      ],
      "metadata": {
        "id": "janBHU6zbTUs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> Chọn đáp án B"
      ],
      "metadata": {
        "id": "SiMHyqlXcQzo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# II. Thực hành (5 câu, mỗi câu 1 điểm)"
      ],
      "metadata": {
        "id": "wKdBSdiEVMYB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sử dụng bộ dữ liệu iris về các loài hoa."
      ],
      "metadata": {
        "id": "E3z22qDRVHFw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTzZLQCBONfQ"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "data = load_iris()\n",
        "\n",
        "X = data['data']\n",
        "y = data['target']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Input layer shape ', X.shape)\n",
        "print('Output layer units: ', 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUF3KcIr6c0P",
        "outputId": "37b81a36-a600-4754-891f-258fd219a5b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input layer shape  (150, 4)\n",
            "Output layer units:  3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hãy thiết kế một mạng MLP với `input layer (4 units) -->  hidden layer 1 (10 units) --> hidden layer 2 (10 units) --> output layer (3) unit` thông qua việc:\n",
        "\n",
        "\n",
        "1) Xác định kích thước các ma trận trọng số $\\Theta_1, \\Theta_2, \\Theta_3$ tại từng layers. Biết rằng tại các layer có xét đến unit bằng 1 cho _hệ số tự do (bias weight)_. Điền shape vào bên dưới:"
      ],
      "metadata": {
        "id": "UgwSJamvdomK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Trả lời:\n",
        "Theta_1.shape = (10, 5) <br>\n",
        "Theta_2.shape = (10, 11) <br>\n",
        "Theta_3.shape = (3, 11) <br>"
      ],
      "metadata": {
        "id": "7oEeYJ-n-nn8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Khởi tạo các ma trận trọng số ngẫu nhiên $\\Theta_1$, $\\Theta_2$, $\\Theta_3$ tương ứng với các layers 1, 2, 3."
      ],
      "metadata": {
        "id": "d7AY7y6kfDv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "theta = [[]] #khởi tạo một list các ma trận trọng số\n",
        "theta.append(np.random.rand(10, 5))   #theta_1 có shape (10,5) \n",
        "theta.append(np.random.rand(10, 11))  #theta_2 có shape (10,11)\n",
        "theta.append(np.random.rand(3, 11))   #theta_3 có shape (3,11)"
      ],
      "metadata": {
        "id": "NtYsBkrr-yQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Viết các hàm chức năng gồm:\n",
        "- activation function để biến đổi phi tuyến: Đầu vào là một véc tơ $\\mathbf{z}$.\n",
        "- dense function là hàm tính output tại mỗi layer: Đầu vào là vector $\\mathbf{a}$ (véc tơ gồm các units layer liền trước) và ma trận trọng số $\\mathbf{W}$ kết nối layer liền trước với layer hiện tại. Lưu ý: Cần áp dụng activation function để biến đổi phi tuyến.\n",
        "- softmax function để tính phân phối xác suất: Đầu vào là một véc tơ $\\mathbf{z}$ của các units tại layer cuối cùng."
      ],
      "metadata": {
        "id": "AVI2Cum_f4Vo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def activation_function(z): #hàm activation function là hàm sigmoid function\n",
        "  return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def dense_function(a, W):\n",
        "  a_expan = np.concatenate(([1],a))             #mở rộng vector a, thêm 1 ở trước\n",
        "  Layer_in = W.dot(a_expan)\n",
        "  Layer_out = activation_function(Layer_in) \n",
        "  return Layer_out\n",
        "\n",
        "def softmax_function(z): #tính phân phối xác suất theo dòng\n",
        "  return (z.T / z.sum(axis = 0)).T"
      ],
      "metadata": {
        "id": "je6n2IFD_cWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) Viến hàm thực hiện quá trình feed forward để tính output cho mô hình từ một véc tơ đầu vào. "
      ],
      "metadata": {
        "id": "s3RXIyq9gU6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feed_forward(X_input, theta = theta, activation = activation_function, n_layers = 3): #mô hình có 3 layers\n",
        "  \"\"\"\n",
        "    Hàm thực hiện feed forward tính output từ mô hình có 1 vector đầu vào \n",
        "    n_layers: số layers của mô hình\n",
        "    X: ma trận đầu vào\n",
        "  \"\"\"\n",
        "  input_list = [X_input]                # list chứa các vecto đầu vào được biến đổi qua từng layer\n",
        "  for i in range(1, n_layers):          # duyệt n-1 layers, layer cuối cùng là output thì chỉ xác định thực hiện dense và softmax\n",
        "    a = dense_function(input_list[-1], W = theta[i])\n",
        "    input_list.append(a)\n",
        "  return softmax_function(dense_function(input_list[-1], W = theta[i]))                 # output_layer, kết quả sau n_layers = 3 lần chuyển đổi"
      ],
      "metadata": {
        "id": "5xJNlj2dDbLy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "outputId": "8fa34238-ad07-49d2-eb65-e0474f772762"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-90aa27918e8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#mô hình có 3 layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \"\"\"\n\u001b[1;32m      3\u001b[0m     \u001b[0mHàm\u001b[0m \u001b[0mthực\u001b[0m \u001b[0mhiện\u001b[0m \u001b[0mfeed\u001b[0m \u001b[0mforward\u001b[0m \u001b[0mtính\u001b[0m \u001b[0moutput\u001b[0m \u001b[0mtừ\u001b[0m \u001b[0mmô\u001b[0m \u001b[0mhình\u001b[0m \u001b[0mcó\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0mvector\u001b[0m \u001b[0mđầu\u001b[0m \u001b[0mvào\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mn_layers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msố\u001b[0m \u001b[0mlayers\u001b[0m \u001b[0mcủa\u001b[0m \u001b[0mmô\u001b[0m \u001b[0mhình\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mma\u001b[0m \u001b[0mtrận\u001b[0m \u001b[0mđầu\u001b[0m \u001b[0mvào\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'theta' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5) Huấn luyện mô hình trên bộ dữ liệu iris bằng keras theo kiến trúc đã xác định như trên với tỷ lệ phân chia tập train/validation là 80/20."
      ],
      "metadata": {
        "id": "0waNs3kyn63q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.layers import Dense, Input, Activation\n",
        "from keras.models import Model, Sequential\n",
        "from sklearn.model_selection import train_test_split \n",
        "from keras.optimizers import Adam\n",
        "# 1. train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
        "# 2. buid model \n",
        "inpt = Input(shape = (4))                   #input layer\n",
        "\n",
        "hidden1 = Dense(10)(inpt)                   #hidden layer 1\n",
        "act1    = Activation('sigmoid')(hidden1)   \n",
        "\n",
        "hidden2 = Dense(10)(act1)                   #hidden layer 2\n",
        "act2    = Activation('sigmoid')(hidden2) \n",
        "\n",
        "hidden3 = Dense(3)(act2)                   #output layer \n",
        "outpt   = Activation('softmax')(hidden3)\n",
        "\n",
        "\n",
        "model   = Model(inputs = [inpt], outputs = [outpt])\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# 3. loss, metrics \n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=keras.optimizers.Adam(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# 4. fit model\n",
        "model.fit(X_train, y_train, \n",
        "          validation_data = (X_test, y_test),\n",
        "          epochs = 100,\n",
        "          batch_size = 8)"
      ],
      "metadata": {
        "id": "Z2zm20OHOLQ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c90837a9-cca3-4aeb-e6b0-d5a4dd5f2c00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_6 (InputLayer)        [(None, 4)]               0         \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 10)                50        \n",
            "                                                                 \n",
            " activation_15 (Activation)  (None, 10)                0         \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 10)                110       \n",
            "                                                                 \n",
            " activation_16 (Activation)  (None, 10)                0         \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 3)                 33        \n",
            "                                                                 \n",
            " activation_17 (Activation)  (None, 3)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 193\n",
            "Trainable params: 193\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "15/15 [==============================] - 1s 21ms/step - loss: 1.1822 - accuracy: 0.3000 - val_loss: 1.0503 - val_accuracy: 0.4667\n",
            "Epoch 2/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.1462 - accuracy: 0.3000 - val_loss: 1.0542 - val_accuracy: 0.4667\n",
            "Epoch 3/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.1222 - accuracy: 0.3000 - val_loss: 1.0626 - val_accuracy: 0.4667\n",
            "Epoch 4/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.1077 - accuracy: 0.3000 - val_loss: 1.0736 - val_accuracy: 0.4667\n",
            "Epoch 5/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.0986 - accuracy: 0.3000 - val_loss: 1.0824 - val_accuracy: 0.4667\n",
            "Epoch 6/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.0940 - accuracy: 0.4500 - val_loss: 1.0931 - val_accuracy: 0.5667\n",
            "Epoch 7/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.0907 - accuracy: 0.4167 - val_loss: 1.1019 - val_accuracy: 0.2000\n",
            "Epoch 8/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.0866 - accuracy: 0.3667 - val_loss: 1.1033 - val_accuracy: 0.2000\n",
            "Epoch 9/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.0850 - accuracy: 0.3667 - val_loss: 1.1021 - val_accuracy: 0.2000\n",
            "Epoch 10/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.0836 - accuracy: 0.3667 - val_loss: 1.1068 - val_accuracy: 0.2000\n",
            "Epoch 11/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.0809 - accuracy: 0.3667 - val_loss: 1.1086 - val_accuracy: 0.2000\n",
            "Epoch 12/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.0795 - accuracy: 0.3667 - val_loss: 1.1016 - val_accuracy: 0.2000\n",
            "Epoch 13/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.0767 - accuracy: 0.3667 - val_loss: 1.1041 - val_accuracy: 0.2000\n",
            "Epoch 14/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.0752 - accuracy: 0.3667 - val_loss: 1.1074 - val_accuracy: 0.2000\n",
            "Epoch 15/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.0719 - accuracy: 0.3667 - val_loss: 1.1039 - val_accuracy: 0.2000\n",
            "Epoch 16/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.0695 - accuracy: 0.3667 - val_loss: 1.1000 - val_accuracy: 0.2000\n",
            "Epoch 17/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.0658 - accuracy: 0.3667 - val_loss: 1.0967 - val_accuracy: 0.2000\n",
            "Epoch 18/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.0638 - accuracy: 0.3667 - val_loss: 1.0946 - val_accuracy: 0.2000\n",
            "Epoch 19/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.0613 - accuracy: 0.3667 - val_loss: 1.0956 - val_accuracy: 0.2000\n",
            "Epoch 20/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.0548 - accuracy: 0.3667 - val_loss: 1.0860 - val_accuracy: 0.2000\n",
            "Epoch 21/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.0506 - accuracy: 0.3667 - val_loss: 1.0771 - val_accuracy: 0.2000\n",
            "Epoch 22/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.0462 - accuracy: 0.3667 - val_loss: 1.0771 - val_accuracy: 0.2000\n",
            "Epoch 23/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.0389 - accuracy: 0.4333 - val_loss: 1.0652 - val_accuracy: 0.5333\n",
            "Epoch 24/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 1.0321 - accuracy: 0.6500 - val_loss: 1.0477 - val_accuracy: 0.6667\n",
            "Epoch 25/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 1.0210 - accuracy: 0.6667 - val_loss: 1.0401 - val_accuracy: 0.6667\n",
            "Epoch 26/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 1.0096 - accuracy: 0.6667 - val_loss: 1.0245 - val_accuracy: 0.6667\n",
            "Epoch 27/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.9985 - accuracy: 0.6667 - val_loss: 1.0025 - val_accuracy: 0.6667\n",
            "Epoch 28/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.9842 - accuracy: 0.6667 - val_loss: 0.9935 - val_accuracy: 0.6667\n",
            "Epoch 29/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.9690 - accuracy: 0.6667 - val_loss: 0.9754 - val_accuracy: 0.6667\n",
            "Epoch 30/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.9540 - accuracy: 0.6667 - val_loss: 0.9563 - val_accuracy: 0.6667\n",
            "Epoch 31/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.9403 - accuracy: 0.6667 - val_loss: 0.9431 - val_accuracy: 0.6667\n",
            "Epoch 32/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.9233 - accuracy: 0.6667 - val_loss: 0.9183 - val_accuracy: 0.6667\n",
            "Epoch 33/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.9080 - accuracy: 0.6667 - val_loss: 0.9006 - val_accuracy: 0.6667\n",
            "Epoch 34/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.8908 - accuracy: 0.6667 - val_loss: 0.8810 - val_accuracy: 0.6667\n",
            "Epoch 35/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.8751 - accuracy: 0.6667 - val_loss: 0.8693 - val_accuracy: 0.6667\n",
            "Epoch 36/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.8573 - accuracy: 0.6667 - val_loss: 0.8458 - val_accuracy: 0.6667\n",
            "Epoch 37/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.8403 - accuracy: 0.6750 - val_loss: 0.8297 - val_accuracy: 0.6667\n",
            "Epoch 38/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.8236 - accuracy: 0.6667 - val_loss: 0.8103 - val_accuracy: 0.6667\n",
            "Epoch 39/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.8074 - accuracy: 0.6833 - val_loss: 0.7891 - val_accuracy: 0.7000\n",
            "Epoch 40/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.7904 - accuracy: 0.6833 - val_loss: 0.7733 - val_accuracy: 0.7000\n",
            "Epoch 41/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.7740 - accuracy: 0.7000 - val_loss: 0.7541 - val_accuracy: 0.7000\n",
            "Epoch 42/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.7581 - accuracy: 0.7083 - val_loss: 0.7352 - val_accuracy: 0.7000\n",
            "Epoch 43/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.7431 - accuracy: 0.7083 - val_loss: 0.7159 - val_accuracy: 0.7000\n",
            "Epoch 44/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.7307 - accuracy: 0.6917 - val_loss: 0.7073 - val_accuracy: 0.6667\n",
            "Epoch 45/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.7139 - accuracy: 0.7000 - val_loss: 0.6814 - val_accuracy: 0.7000\n",
            "Epoch 46/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.6991 - accuracy: 0.7250 - val_loss: 0.6640 - val_accuracy: 0.7667\n",
            "Epoch 47/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.6854 - accuracy: 0.7167 - val_loss: 0.6524 - val_accuracy: 0.7000\n",
            "Epoch 48/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.6723 - accuracy: 0.7167 - val_loss: 0.6350 - val_accuracy: 0.7667\n",
            "Epoch 49/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.6600 - accuracy: 0.7167 - val_loss: 0.6207 - val_accuracy: 0.7333\n",
            "Epoch 50/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.6476 - accuracy: 0.7333 - val_loss: 0.6057 - val_accuracy: 0.7667\n",
            "Epoch 51/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.6354 - accuracy: 0.7667 - val_loss: 0.5936 - val_accuracy: 0.7667\n",
            "Epoch 52/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.6244 - accuracy: 0.7667 - val_loss: 0.5799 - val_accuracy: 0.7667\n",
            "Epoch 53/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.6137 - accuracy: 0.7667 - val_loss: 0.5681 - val_accuracy: 0.7667\n",
            "Epoch 54/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.6031 - accuracy: 0.7750 - val_loss: 0.5561 - val_accuracy: 0.8000\n",
            "Epoch 55/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.5934 - accuracy: 0.8000 - val_loss: 0.5446 - val_accuracy: 0.8667\n",
            "Epoch 56/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.5839 - accuracy: 0.8083 - val_loss: 0.5341 - val_accuracy: 0.8667\n",
            "Epoch 57/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.5749 - accuracy: 0.8250 - val_loss: 0.5220 - val_accuracy: 0.8667\n",
            "Epoch 58/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.5666 - accuracy: 0.8250 - val_loss: 0.5111 - val_accuracy: 0.8667\n",
            "Epoch 59/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.5583 - accuracy: 0.8667 - val_loss: 0.5014 - val_accuracy: 0.8667\n",
            "Epoch 60/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.5492 - accuracy: 0.8500 - val_loss: 0.4953 - val_accuracy: 0.8667\n",
            "Epoch 61/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.5421 - accuracy: 0.8250 - val_loss: 0.4863 - val_accuracy: 0.8667\n",
            "Epoch 62/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.5343 - accuracy: 0.8417 - val_loss: 0.4781 - val_accuracy: 0.8667\n",
            "Epoch 63/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.5294 - accuracy: 0.8917 - val_loss: 0.4666 - val_accuracy: 0.9667\n",
            "Epoch 64/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.5204 - accuracy: 0.8750 - val_loss: 0.4628 - val_accuracy: 0.8667\n",
            "Epoch 65/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.5136 - accuracy: 0.8917 - val_loss: 0.4526 - val_accuracy: 0.9667\n",
            "Epoch 66/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.5070 - accuracy: 0.9167 - val_loss: 0.4449 - val_accuracy: 1.0000\n",
            "Epoch 67/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.5004 - accuracy: 0.9083 - val_loss: 0.4386 - val_accuracy: 1.0000\n",
            "Epoch 68/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4946 - accuracy: 0.8917 - val_loss: 0.4338 - val_accuracy: 0.9333\n",
            "Epoch 69/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.4886 - accuracy: 0.9250 - val_loss: 0.4242 - val_accuracy: 1.0000\n",
            "Epoch 70/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.4823 - accuracy: 0.9167 - val_loss: 0.4200 - val_accuracy: 1.0000\n",
            "Epoch 71/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.4762 - accuracy: 0.9167 - val_loss: 0.4136 - val_accuracy: 1.0000\n",
            "Epoch 72/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4706 - accuracy: 0.9417 - val_loss: 0.4069 - val_accuracy: 1.0000\n",
            "Epoch 73/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4661 - accuracy: 0.9167 - val_loss: 0.4023 - val_accuracy: 1.0000\n",
            "Epoch 74/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4614 - accuracy: 0.9500 - val_loss: 0.3934 - val_accuracy: 1.0000\n",
            "Epoch 75/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4543 - accuracy: 0.9583 - val_loss: 0.3892 - val_accuracy: 1.0000\n",
            "Epoch 76/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4491 - accuracy: 0.9500 - val_loss: 0.3858 - val_accuracy: 1.0000\n",
            "Epoch 77/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4440 - accuracy: 0.9500 - val_loss: 0.3804 - val_accuracy: 1.0000\n",
            "Epoch 78/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4395 - accuracy: 0.9500 - val_loss: 0.3738 - val_accuracy: 1.0000\n",
            "Epoch 79/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4351 - accuracy: 0.9583 - val_loss: 0.3665 - val_accuracy: 1.0000\n",
            "Epoch 80/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4283 - accuracy: 0.9583 - val_loss: 0.3644 - val_accuracy: 1.0000\n",
            "Epoch 81/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4243 - accuracy: 0.9583 - val_loss: 0.3595 - val_accuracy: 1.0000\n",
            "Epoch 82/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4203 - accuracy: 0.9583 - val_loss: 0.3555 - val_accuracy: 1.0000\n",
            "Epoch 83/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.4138 - accuracy: 0.9583 - val_loss: 0.3495 - val_accuracy: 1.0000\n",
            "Epoch 84/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.4092 - accuracy: 0.9583 - val_loss: 0.3441 - val_accuracy: 1.0000\n",
            "Epoch 85/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4046 - accuracy: 0.9583 - val_loss: 0.3385 - val_accuracy: 1.0000\n",
            "Epoch 86/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4022 - accuracy: 0.9583 - val_loss: 0.3342 - val_accuracy: 1.0000\n",
            "Epoch 87/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.3964 - accuracy: 0.9583 - val_loss: 0.3346 - val_accuracy: 1.0000\n",
            "Epoch 88/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.3911 - accuracy: 0.9583 - val_loss: 0.3266 - val_accuracy: 1.0000\n",
            "Epoch 89/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.3859 - accuracy: 0.9583 - val_loss: 0.3223 - val_accuracy: 1.0000\n",
            "Epoch 90/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.3816 - accuracy: 0.9583 - val_loss: 0.3182 - val_accuracy: 1.0000\n",
            "Epoch 91/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.3773 - accuracy: 0.9583 - val_loss: 0.3140 - val_accuracy: 1.0000\n",
            "Epoch 92/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.3752 - accuracy: 0.9583 - val_loss: 0.3078 - val_accuracy: 1.0000\n",
            "Epoch 93/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.3678 - accuracy: 0.9583 - val_loss: 0.3053 - val_accuracy: 1.0000\n",
            "Epoch 94/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.3642 - accuracy: 0.9583 - val_loss: 0.3016 - val_accuracy: 1.0000\n",
            "Epoch 95/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.3599 - accuracy: 0.9583 - val_loss: 0.2978 - val_accuracy: 1.0000\n",
            "Epoch 96/100\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.3559 - accuracy: 0.9583 - val_loss: 0.2924 - val_accuracy: 1.0000\n",
            "Epoch 97/100\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.3537 - accuracy: 0.9583 - val_loss: 0.2906 - val_accuracy: 1.0000\n",
            "Epoch 98/100\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.3465 - accuracy: 0.9583 - val_loss: 0.2848 - val_accuracy: 1.0000\n",
            "Epoch 99/100\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.3437 - accuracy: 0.9583 - val_loss: 0.2810 - val_accuracy: 1.0000\n",
            "Epoch 100/100\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.3393 - accuracy: 0.9583 - val_loss: 0.2767 - val_accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6a934b50a0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.evaluate(X_test, y_test, batch_size=8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yjg5TI2oXUdo",
        "outputId": "fa0d8fc6-44ff-4be7-ceae-a2b93134fac8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 3ms/step - loss: 0.2767 - accuracy: 1.0000\n"
          ]
        }
      ]
    }
  ]
}