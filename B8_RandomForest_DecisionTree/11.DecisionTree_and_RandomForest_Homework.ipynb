{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"C8Ynejw36epa"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["# I. Lý thuyết (5đ - 10 câu, mỗi câu 0.5đ)"],"metadata":{"id":"fLg0oNYR78oz"}},{"cell_type":"markdown","source":["1) Thuật toán Decision Tree được sử dụng cho những lớp bài toán nào?\n","\n","A. Bài toán phân lớp (Classification)\n","\n","B. Bài toán hồi quy (Regression)\n","\n","C. Bài toán phân cụm (Clustering)\n","\n","D. A và B"],"metadata":{"id":"b9SmBkpI8E8u"}},{"cell_type":"markdown","source":["2) Độ đo nào sau đây được dùng để so sánh về mức độ entropy trước và sau khi chia nhánh tại một node bởi một thuộc tính\n","\n","A. Information Gain \n","\n","B. Gini Index \n","\n","C. Entropy\n","\n","D. Cả 3 phương án trên"],"metadata":{"id":"Zhgo5VEn8Fd2"}},{"cell_type":"markdown","source":["3) Thuật toán Decision Tree có thể  sử dụng các độ đo nào sau đây để đánh giá chất lượng phân loại tại một node lá.\n","\n","A. Information Gain \n","\n","B. Gini Index \n","\n","C. Entropy\n","\n","D. B và C"],"metadata":{"id":"ixVn848X8Fum"}},{"cell_type":"markdown","source":["4) Một bài toán phân loại với $C$ lớp có phân phối xác suất tại đầu ra là một véc tơ $\\mathbf{p} = [p_1, p_2, \\dots, p_C]$. Chỉ số Gini được sử dụng để đo lường chất lượng phân loại tại node lá:\n","\n","$$\\text{Gini} = 1-\\sum_{i=1}^{C} p_i^2$$\n","\n","Gía trị lớn nhất của chỉ số Gini có thể là bao nhiêu?\n","\n","A. 0.1 \n","\n","B. $1 - \\frac{1}{C}$ với C là số nhãn (label) của bài toán \n","\n","C. 0.5\n","\n","D. Không xác định được"],"metadata":{"id":"K65vUrAF8F_v"}},{"cell_type":"markdown","source":["5) Chọn khẳng định đúng trong các khẳng định sau đây\n","\n","A. Thuật toán ID3 dùng cho các features có tính categorical. Thuật toán C4.5 dùng cho các feature có cả tính categorical và numeric.\n","\n","B. Cây quyết định trong thuật toán CART là cây nhị phân. \n","\n","C. Các thuật toán Decision Tree thường có xu hướng high variance và low bias. Do đó thường xảy ra hiện tượng overfitting.\n","\n","D. Tất cả các phương án trên."],"metadata":{"id":"6YvgNiMm8YMa"}},{"cell_type":"markdown","source":["6) Phương pháp Ensemble Learning có nghĩa là gì?\n","\n","A. Sử dụng kết hợp kết quả dự báo từ nhiều models thành phần để dự báo cho một quan sát.\n","\n","B. Là phương pháp tạo tập dữ liệu con thông qua lấy mẫu lặp lại các quan sát thuộc tập mẫu tổng thể.\n","\n","C. Là phương pháp phân chia tập dữ liệu thành $k$ phần. Huấn luyện mô hình trên $k-1$ phần và đánh giá sai số trên phần còn lại.  \n","\n","D. Tất cả các phương án trên"],"metadata":{"id":"ClaVO85A8YPS"}},{"cell_type":"markdown","source":["7) Thuật toán Random Forest sử dụng những kỹ thuật nào?\n","\n","A. Ensemble Learning. \n","\n","B. Boostrapping.\n","\n","C. Bagging \n","\n","D. Tất cả các phương án trên"],"metadata":{"id":"8aRIXiqc8YSK"}},{"cell_type":"markdown","source":["8) Trong các khẳng định sau, khẳng định nào là sai?\n","\n","- 1. Random Forest là mô hình kết hợp từ nhiều mô hình Decision Tree. \n","- 2. Đánh giá mô hình `Random Forest` theo `phương pháp Bagging` sẽ lựa chọn các mẫu nằm ngoài các tập huấn luyện con để thực hiện đánh giá.\n","- 3. Một cây quyêt định (Decision Tree) trong Random Forest được huấn luyện trên toàn bộ các mẫu của tập training.\n","- 4. Trong package scikit-learn, một cây quyêt định (Decision Tree) trong thuật toán `Random Forest` có thể giới hạn số lượng features đầu vào thông qua `max_features`\n","- 5. Mức độ quan trọng của một biến đầu vào có thể được đánh giá thông qua mô hình `Random Forest` nhờ độ sâu trung bình trên toàn bộ cây quyết định con.\n","\n","A. 1 \n","\n","B. 2\n","\n","C. 3\n","\n","D. 4 và 5"],"metadata":{"id":"pabOnZcg8YVH"}},{"cell_type":"markdown","source":["9) Đâu là khẳng định đúng?\n","\n","A. Kết quả dự báo từ phương pháp Ensemble Learning thường có tính chất `low variance` và `low bias`.\n","\n","B. Điểm khác biệt giữa mô hình Random Forest và mô hình Decision Tree đó chính là Ensemble Learning và và Boostrapping.  \n","\n","C. Trong class RandomForestClassifier của package scikit-learn thì chi phí huấn luyện của mô hình Random Forest càng lớn nếu như số lượng `n_estimators` càng lớn.\n","\n","D. Cả ba đáp án trên."],"metadata":{"id":"-xfw_WIx8YXr"}},{"cell_type":"markdown","source":["10. Tại một node của cây quyết định (Decision Tree) trong bài toán phân loại nhị phân có số lượng các quan sát nhãn 0 và 1 lần lượt là 4 và 6. Giá trị entropy tại node đó là \n","\n","A. $- 0.6 \\log(0.6) - 0.4 \\log(0.4)$\n","\n","B. $0.6 \\log(0.6) + 0.4 \\log(0.4)$\n","\n","C. $- 0.6 \\log(0.6) - (1 - 0.4) \\log(1 - 0.4)$\n","\n","D. $0.6 \\log(0.6) + (1 - 0.4) \\log(1 - 0.4)$"],"metadata":{"id":"ALxl5SGX8Yaz"}},{"cell_type":"code","source":[],"metadata":{"id":"2Je2GAVL8Wwf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# II. Thực hành (5 câu, mỗi câu 1đ)"],"metadata":{"id":"xtMLDxGw8iq5"}},{"cell_type":"markdown","source":["Cho bộ dữ liệu \n","\n","\n","| Cap Color | Stalk Shape | Solitary | Edible |\n","|:---------:|:-----------:|:--------:|:------:|\n","|   Brown   |   Tapering  |    Yes   |    1   |\n","|   Brown   |  Enlarging  |    Yes   |    1   |\n","|   Brown   |  Enlarging  |    No    |    0   |\n","|   Brown   |  Enlarging  |    No    |    0   |\n","|   Brown   |   Tapering  |    Yes   |    1   |\n","|    Red    |   Tapering  |    Yes   |    0   |\n","|    Red    |  Enlarging  |    No    |    0   |\n","|   Brown   |  Enlarging  |    Yes   |    1   |\n","|    Red    |   Tapering  |    No    |    1   |\n","|   Brown   |  Enlarging  |    No    |    0   |\n","\n","\n","-  Có 10 quan sát về các loại nấm. Với mỗi loại \n","    - Có 3 features\n","        - Cap Color (`Brown` or `Red`) - Màu nắp nấm \n","        - Stalk Shape (`Tapering` or `Enlarging`) - Hình dạng cây nhấm (thuôn gọn - Tapering hoặc mở rộng - Enlarging)\n","        - Solitary (`Yes` or `No`) - Sinh sô đơn lẻ hay theo cụm\n","    - Label\n","        - Edible (`1` indicating yes or `0` indicating poisonous) - Dự đoán ăn được hay không?\n","\n","Một cách đơn giản, ta có thể dùng one-hot encoding để biểu diễn dữ liệu trên dưới dạng vector \n","\n","| Brown Cap | Tapering Stalk Shape | Solitary | Edible |\n","|:---------:|:--------------------:|:--------:|:------:|\n","|     1     |           1          |     1    |    1   |\n","|     1     |           0          |     1    |    1   |\n","|     1     |           0          |     0    |    0   |\n","|     1     |           0          |     0    |    0   |\n","|     1     |           1          |     1    |    1   |\n","|     0     |           1          |     1    |    0   |\n","|     0     |           0          |     0    |    0   |\n","|     1     |           0          |     1    |    1   |\n","|     0     |           1          |     0    |    1   |\n","|     1     |           0          |     0    |    0   |\n","\n","\n","Trong đó, \n","- `X_train` contains three features for each example \n","    - Brown Color (A value of `1` indicates \"Brown\" cap color and `0` indicates \"Red\" cap color)\n","    - Tapering Shape (A value of `1` indicates \"Tapering Stalk Shape\" and `0` indicates \"Enlarging\" stalk shape)\n","    - Solitary  (A value of `1` indicates \"Yes\" and `0` indicates \"No\")\n","\n","- `y_train` is whether the mushroom is edible \n","    - `y = 1` indicates edible\n","    - `y = 0` indicates poisonous\n","\n"],"metadata":{"id":"97J6_hntV9i-"}},{"cell_type":"code","source":["import numpy as np\n","\n","X_train = np.array(\n","    [[1,1,1],\n","     [1,0,1],\n","     [1,0,0],\n","     [1,0,0],\n","     [1,1,1],\n","     [0,1,1],\n","     [0,0,0],\n","     [1,0,1],\n","     [0,1,0],\n","     [1,0,0]]\n",")\n","\n","y_train = np.array([1,1,0,0,1,0,0,1,1,0])"],"metadata":{"id":"kd5BbT_M8nNY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["1) Viết hàm tính entropy trong đó \n","- Input: `y` là một `array` dạng véc tơ tần suất các nhãn 0, 1 tại node hiện tại.\n","- Output: Giá trị entropy tương ứng với `y`"],"metadata":{"id":"npVngvOJazC5"}},{"cell_type":"code","source":["def compute_entropy(y):\n","    \"\"\"\n","    Computes the entropy for \n","    \n","    Args:\n","       y (ndarray): Numpy array indicating whether each example at a node is\n","           edible (`1`) or poisonous (`0`)\n","       \n","    Returns:\n","        entropy (float): Entropy at that node\n","        \n","    \"\"\"\n","    # You need to return the following variables correctly\n","    entropy = 0.\n","    \n","    ### START CODE HERE ###\n","    \n","    \n","    ### END CODE HERE ###        \n","    \n","    return entropy"],"metadata":{"id":"jCbac7jMazMl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Test \n","# Compute entropy at the root node (i.e. with all examples)\n","# Since we have 5 edible and 5 non-edible mushrooms, the entropy should be 1\"\n","\n","print(\"Entropy at root node: \", compute_entropy(y_train))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RsfC8CT8buv-","outputId":"40ccf2ad-d885-489e-8c2d-8c694935ba2d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Entropy at root node:  1.0\n"]}]},{"cell_type":"markdown","source":["2) Tại một node chúng ta cần lựa chọn một feature để quyết định quan sát sẽ rơi vào nhánh True hoặc False. Giả sử nhánh True ở bên trái nhận giá trị 1 và nhánh False ở bên phải nhận giá trị 0. Viết hàm chia nhánh tại một node tương ứng với `feature` xác định trước.\n","\n","Trong đó:\n","\n","- Input:\n","  - X: Ma trận đầu vào\n","  - node_indices: list indices của các quan sát trước khi chia nhánh.\n","  - feature: indice column của feature được lựa chọn để chia nhánh.\n","- Output:\n","  - left_indices: list indices của nhánh bên trái.\n","  - right_indices: list indices của nhánh bên phải.\n"],"metadata":{"id":"xHLiExoITZS4"}},{"cell_type":"code","source":["def split_dataset(X, node_indices, feature):\n","    \"\"\"\n","    Splits the data at the given node into\n","    left and right branches\n","    \n","    Args:\n","        X (ndarray):             Data matrix of shape(n_samples, n_features)\n","        node_indices (list):     List containing the active indices. I.e, the samples being considered at this step.\n","        feature (int):           Column Index of feature to split on\n","    \n","    Returns:\n","        left_indices (list):     Indices with feature value == 1\n","        right_indices (list):    Indices with feature value == 0\n","    \"\"\"\n","    \n","    # You need to return the following variables correctly\n","    left_indices = []\n","    right_indices = []\n","    \n","    ### START CODE HERE ###\n","    for i in node_indices:   \n","        continue # write your code here\n","\n","    ### END CODE HERE ###\n","        \n","    return left_indices, right_indices"],"metadata":{"id":"VQ5WmAiRTZax"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Test\n","root_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n","\n","# Feel free to play around with these variables\n","# The dataset only has three features, so this value can be 0 (Brown Cap), 1 (Tapering Stalk Shape) or 2 (Solitary)\n","feature = 0\n","\n","left_indices, right_indices = split_dataset(X_train, root_indices, feature)\n","\n","print(\"Left indices: \", left_indices)\n","print(\"Right indices: \", right_indices)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QlDVoUm4dSdr","outputId":"b83fe64c-a539-4114-b1d4-3652d8de0e66"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Left indices:  [0, 1, 2, 3, 4, 7, 9]\n","Right indices:  [5, 6, 8]\n"]}]},{"cell_type":"markdown","source":["**Expected Output**:\n","```\n","Left indices:  [0, 1, 2, 3, 4, 7, 9]\n","Right indices:  [5, 6, 8]\n","```"],"metadata":{"id":"aQk1afHldXgr"}},{"cell_type":"markdown","source":["3) Viết hàm tính toán Information Gain tại mỗi node lá.\n","\n","Trong đó:\n","\n","- Input:\n","  - X: Ma trận đầu vào.\n","  - y: Biến mục tiêu.\n","  - node_indices: list indices của các quan sát thuộc node hiện tại.\n","  - feature: Column indice của feature được lựa chọn để rẽ nhánh. \n","- Output:\n","  - Giá trị Information Gain.\n"],"metadata":{"id":"sH1YLyMUTZlR"}},{"cell_type":"code","source":["def compute_information_gain(X, y, node_indices, feature):\n","    \n","    \"\"\"\n","    Compute the information of splitting the node on a given feature\n","    \n","    Args:\n","        X (ndarray):            Data matrix of shape(n_samples, n_features)\n","        y (array like):         list or ndarray with n_samples containing the target variable\n","        node_indices (ndarray): List containing the active indices. I.e, the samples being considered in this step.\n","        feature (int):          Column Index of feature to split on\n","    Returns:\n","        cost (float):        Cost computed\n","    \n","    \"\"\"    \n","    # Split dataset\n","    left_indices, right_indices = split_dataset(X, node_indices, feature)\n","    \n","    # Some useful variables\n","    X_node, y_node = X[node_indices], y[node_indices]\n","    X_left, y_left = X[left_indices], y[left_indices]\n","    X_right, y_right = X[right_indices], y[right_indices]\n","    \n","    # You need to return the following variables correctly\n","    information_gain = 0\n","    \n","    ### START CODE HERE ###\n","    \n","    \n","\n","    ### END CODE HERE ###  \n","    \n","    return information_gain"],"metadata":{"id":"MckTRvZeTZy8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["info_gain0 = compute_information_gain(X_train, y_train, root_indices, feature=0)\n","print(\"Information Gain from splitting the root on brown cap: \", info_gain0)\n","\n","info_gain1 = compute_information_gain(X_train, y_train, root_indices, feature=1)\n","print(\"Information Gain from splitting the root on tapering stalk shape: \", info_gain1)\n","\n","info_gain2 = compute_information_gain(X_train, y_train, root_indices, feature=2)\n","print(\"Information Gain from splitting the root on solitary: \", info_gain2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x3C6Am_nd3HW","outputId":"c7d36104-c2db-418d-f960-5d657df40dd3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Information Gain from splitting the root on brown cap:  0.034851554559677034\n","Information Gain from splitting the root on tapering stalk shape:  0.12451124978365313\n","Information Gain from splitting the root on solitary:  0.2780719051126377\n"]}]},{"cell_type":"markdown","source":["**Expected Output**:\n","```\n","Information Gain from splitting the root on brown cap:  0.034851554559677034\n","Information Gain from splitting the root on tapering stalk shape:  0.12451124978365313\n","Information Gain from splitting the root on solitary:  0.2780719051126377\n","```"],"metadata":{"id":"u_vFRes6eTXr"}},{"cell_type":"markdown","source":["**Mở rộng**: Để tạo cây quyết định thì bạn có thể tham khảo code bên dưới thông qua việc áp dụng các hàm `split_dataset()` và `compute_information_gain()` ở trên. Phần này không chấm điểm.\n"],"metadata":{"id":"XaW60xPdEGsk"}},{"cell_type":"code","source":["\n","def get_best_split(X, y, node_indices):   \n","    \"\"\"\n","    Returns the optimal feature and threshold value\n","    to split the node data \n","    \n","    Args:\n","        X (ndarray):            Data matrix of shape(n_samples, n_features)\n","        y (array like):         list or ndarray with n_samples containing the target variable\n","        node_indices (ndarray): List containing the active indices. I.e, the samples being considered in this step.\n","\n","    Returns:\n","        best_feature (int):     The index of the best feature to split\n","    \"\"\"    \n","    \n","    # Some useful variables\n","    num_features = X.shape[1]\n","    \n","    # You need to return the following variables correctly\n","    best_feature = -1\n","    \n","    max_info_gain = 0\n","    # Iterate through all features\n","    for feature in range(num_features): \n","\n","       # Your code here to compute the information gain from splitting on this feature\n","       info_gain = compute_information_gain(X, y, node_indices, feature)\n","\n","       # If the information gain is larger than the max seen so far\n","       if info_gain > max_info_gain:  \n","            max_info_gain = info_gain\n","            best_feature = feature  \n","   \n","    return best_feature\n","\n","tree = []\n","def build_tree_recursive(X, y, node_indices, branch_name, max_depth, current_depth):\n","    \"\"\"\n","    Build a tree using the recursive algorithm that split the dataset into 2 subgroups at each node.\n","    This function just prints the tree.\n","    \n","    Args:\n","        X (ndarray):            Data matrix of shape(n_samples, n_features)\n","        y (array like):         list or ndarray with n_samples containing the target variable\n","        node_indices (ndarray): List containing the active indices. I.e, the samples being considered in this step.\n","        branch_name (string):   Name of the branch. ['Root', 'Left', 'Right']\n","        max_depth (int):        Max depth of the resulting tree. \n","        current_depth (int):    Current depth. Parameter used during recursive call.\n","   \n","    \"\"\" \n","\n","    # Maximum depth reached - stop splitting\n","    if current_depth == max_depth:\n","        formatting = \" \"*current_depth + \"-\"*current_depth\n","        print(formatting, \"%s leaf node with indices\" % branch_name, node_indices)\n","        return\n","   \n","    # Otherwise, get best split and split the data\n","    # Get the best feature and threshold at this node\n","    best_feature = get_best_split(X, y, node_indices) \n","    \n","    formatting = \"-\"*current_depth\n","    print(\"%s Depth %d, %s: Split on feature: %d\" % (formatting, current_depth, branch_name, best_feature))\n","    \n","    # Split the dataset at the best feature\n","    left_indices, right_indices = split_dataset(X, node_indices, best_feature)\n","    tree.append((left_indices, right_indices, best_feature))\n","    \n","    # continue splitting the left and the right child. Increment current depth\n","    build_tree_recursive(X, y, left_indices, \"Left\", max_depth, current_depth+1)\n","    build_tree_recursive(X, y, right_indices, \"Right\", max_depth, current_depth+1)\n","\n","\n","# Test function \n","build_tree_recursive(X_train, y_train, root_indices, \"Root\", max_depth=2, current_depth=0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LrjQZRM-e6It","outputId":"33a21836-51e8-44a4-e18d-08e6f486091c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" Depth 0, Root: Split on feature: 2\n","- Depth 1, Left: Split on feature: 0\n","  -- Left leaf node with indices [0, 1, 4, 7]\n","  -- Right leaf node with indices [5]\n","- Depth 1, Right: Split on feature: 1\n","  -- Left leaf node with indices [8]\n","  -- Right leaf node with indices [2, 3, 6, 9]\n"]}]},{"cell_type":"markdown","source":["4) Sử dụng dữ bộ dữ liệu `breast cancer` có sẵn trên `scikit-learn` để dự đoán khả năng ung thư  vú của một người. Sử dụng mô hình Decision Tree với các bước \n","- Chia dữ liệu thành 2 tập là train và test\n","- EDA \n","- Xây dựng pipeline huấn luyện mô hình \n","- Đánh giá kết quả trên tập test"],"metadata":{"id":"OqclGZo0Ta-S"}},{"cell_type":"code","source":["from sklearn import datasets\n","\n","X, y = datasets.load_breast_cancer(return_X_y=True)\n","X, y"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5rzMw63rTbJh","outputId":"58cebdfd-5684-4e21-bdcb-41752d1a7178"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,\n","         1.189e-01],\n","        [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,\n","         8.902e-02],\n","        [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,\n","         8.758e-02],\n","        ...,\n","        [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,\n","         7.820e-02],\n","        [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,\n","         1.240e-01],\n","        [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,\n","         7.039e-02]]),\n"," array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n","        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n","        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n","        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n","        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n","        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n","        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n","        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n","        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n","        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n","        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n","        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n","        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n","        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n","        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n","        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n","        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n","        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]))"]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","source":["5) Thực hiện huấn luyện mô hình theo các bước tương tự như câu 4 trên mô hình Random Forest. Có nhận xét gì về kết quả của 2 mô hình (Decision Tree vs Random Forest)?"],"metadata":{"id":"GH2bWYjsTbRp"}},{"cell_type":"code","source":[],"metadata":{"id":"cK2-BLL7Tjc2"},"execution_count":null,"outputs":[]}]}