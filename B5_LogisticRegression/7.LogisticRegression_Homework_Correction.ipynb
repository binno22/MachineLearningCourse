{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# I. Lý thuyết"],"metadata":{"id":"_NWsO-83PoU9"}},{"cell_type":"markdown","source":["1) Xác suất dự báo của mô hình hồi qui Logistic được xây dựng dựa trên hàm Sigmoid có công thức như thế nào ?\n","\n","A. $\\sigma(x) = \\frac{1}{1+e^{-x}}$\n","\n","B. $\\sigma(x) = \\frac{e^{x}}{1+e^{x}}$\n","\n","C. $\\sigma(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$\n","\n","D. Cả A và B"],"metadata":{"id":"5SBg7TzVPrFY"}},{"cell_type":"markdown","source":["D"],"metadata":{"id":"O_eGODnMlTMt"}},{"cell_type":"markdown","source":["2) Phát biểu nào sau đây là chính xác về hàm Sigmoid\n","\n","A. Hàm Sigmoid là hàm số đồng biến.\n","\n","B. Hàm Sigmoid là hàm lồi.\n","\n","C. Hàm Sigmoid có giá trị nằm trong khoảng (0, 1).\n","\n","D. Cả ba đáp án trên."],"metadata":{"id":"ktUs4NoqQWju"}},{"cell_type":"markdown","source":["D"],"metadata":{"id":"97F9I3g5liCk"}},{"cell_type":"markdown","source":["3) Lý do Linear Regression thường không được sử dụng trong bài toán phân loại là gì ?\n","\n","A. Vì các giá trị dự báo có thể nằm ngoài khoảng (0, 1).\n","\n","B. Hồi qui Linear Regression không có đường biên phân chia.\n","\n","C. Mô hình dễ nhạy cảm với outliers khiến các dự báo bị sai.\n","\n","D. Đáp án A và C"],"metadata":{"id":"PNueGbpqRL3F"}},{"cell_type":"markdown","source":["D"],"metadata":{"id":"oB4gjgnbl-W8"}},{"cell_type":"markdown","source":["4) Nếu chọn ngưỡng xác suất là 0.5 để quyết định nhãn dự báo thì đường biên phân chia của hồi qui Logistic là gì ?\n","\n","A. Một đường thẳng có phương trình $y=ax+b$\n","\n","B. Một siêu phẳng có phương trình $\\mathbf{w}^{\\intercal}\\mathbf{x} = 0$\n","\n","C. Một hình cầu. \n","\n","D. Một đường biên có quĩ đạo ellipse."],"metadata":{"id":"qf8IYbE6c-3Q"}},{"cell_type":"markdown","source":["B"],"metadata":{"id":"KUndLWUTmv6Y"}},{"cell_type":"markdown","source":["5) Trong hồi qui Logistic tỷ lệ Odd Ratio được xác định như sau:\n","\n","$$\\text{Odd Ratio} = \\frac{P(y=1|\\mathbf{x}; \\mathbf{w})}{P(y=0|\\mathbf{x}; \\mathbf{w})} = \\frac{P(y=1|\\mathbf{x}, \\mathbf{w})}{1-P(y=1|\\mathbf{x}, \\mathbf{w})} = e^{\\mathbf{w}^{\\intercal}\\mathbf{x}}$$\n","\n","Tỷ lệ này có ý nghĩa như thế nào?\n","\n","A. Odd ratio là 1 chỉ số đo lường tỷ lệ xác suất giữa trường hợp _tích cực_ và _tiêu cực_ được dự báo từ mô hình hồi quy logistic.\n","\n","B. Căn cứ vào Odd Ratio, ta nhận biết được xác suất _tích cực_ hay _tiêu cực_ lớn hơn. \n","\n","C. Odd ratio càng lớn thì khả năng dự đoán có nhãn tích cực (positive) càng cao. Ngược lại, Odd ratio càng nhỏ thì khả năng dự đoán có nhãn tiêu cực (negative) càng cao.\n","\n","D. Cả ba đáp án trên."],"metadata":{"id":"JKw5fHsHezA5"}},{"cell_type":"markdown","source":["D"],"metadata":{"id":"l7BP44VbnnBW"}},{"cell_type":"markdown","source":["6) Tại sao chúng ta không sử dụng hàm loss function là MSE (Mean Square Error) cho hồi qui Logistic?\n","\n","A. Vì hàm Loss Function MSE có chi phí tính toán lớn.\n","\n","B. Vì hàm Loss Function MSE không phải là một hàm lồi nên quá trình hội tụ tới nghiệm là global optimum khó khăn.\n","\n","C. Vì hàm Loss Function MSE không thể tính được đạo hàm bậc nhất.\n","\n","D. Vì hàm Loss Function MSE không thể đo được sai số giữa xác suất dự báo với nhãn 0, 1."],"metadata":{"id":"1jx0Qu6ehir7"}},{"cell_type":"markdown","source":["\n","B"],"metadata":{"id":"oRb4e1V2oNoE"}},{"cell_type":"markdown","source":["7) Đạo hàm của hàm Sigmoid có giá trị như thế nào ?\n","\n","A. $\\sigma(x)' = \\tanh(x)(1-\\sigma(x))$\n","\n","B. $\\sigma(x)' = \\sigma(x)(1-\\sigma(x))$\n","\n","C. $\\sigma(x)' = \\sigma(x)^2(1-\\sigma(x))$\n","\n","D. $\\sigma(x)' = \\sigma(x)(1-\\sigma(x))^2$"],"metadata":{"id":"fAzkcPjnhWv2"}},{"cell_type":"markdown","source":["B"],"metadata":{"id":"3D446ytipK8I"}},{"cell_type":"markdown","source":["8) Phương pháp nào thường được sử dụng để tìm nghiệm cho hồi qui Logistic?\n","\n","A. Sử dụng phương pháp _hạ dốc_ (_gradient descent_).\n","\n","B. Đạo hàm bậc nhất.\n","\n","C. Sử dụng công thức Newton để tìm nghiệm.\n","\n","D. Sử dụng khai triển Taylor."],"metadata":{"id":"F1RdfbbwjnSp"}},{"cell_type":"markdown","source":[" A"],"metadata":{"id":"QH3IXqISqazg"}},{"cell_type":"markdown","source":["9) Hàm loss function Cross Entropy có dạng như thế nào ? Biết rằng trong công thức bên dưới thì $y_i$ là nhãn và $\\hat{y_i} = p(y=1|\\mathbf{x}_i; \\mathbf{w})$:\n","\n","A. $\\mathcal{L}(\\mathbf{w}) = \\sum_{i=1}^n [y_i\\log(\\hat{y_i}) + (1-y_i)\\log{(1-\\hat{y_i})}]$\n","\n","B. $\\mathcal{L}(\\mathbf{w}) = -\\sum_{i=1}^n [y_i\\log(\\hat{y_i}) + (1-y_i)\\log{(1-\\hat{y_i})}]$\n","\n","C. $\\mathcal{L}(\\mathbf{w}) = \\sum_{i=1}^n y_i\\log(\\frac{y_i}{\\hat{y_i}})$\n","\n","D. $\\mathcal{L}(\\mathbf{w}) = \\sum_{i=1}^n -(1-\\hat{y_i})^{\\gamma}\\log(\\hat{y_i})$"],"metadata":{"id":"BlyiewhomCXv"}},{"cell_type":"markdown","source":["B"],"metadata":{"id":"isSRFMFLsUnc"}},{"cell_type":"markdown","source":["10) Phát biểu nào sau đây là đúng về hàm mất mát Cross Entropy?\n","\n","A. Đo lường khoảng cách giữa phân phối xác suất thực tế và phân phối xác suất dự báo.\n","\n","B. Đo lường độ chính xác của mô hình.\n","\n","C. Đo lường sai số của mô hình.\n","\n","D. Đo lường mức độ giống nhau giữa phân phối xác suất thực tế và phân phối xác suất dự báo."],"metadata":{"id":"tKVDn_s9owz5"}},{"cell_type":"markdown","source":["D"],"metadata":{"id":"6M6ahqK7vPqv"}},{"cell_type":"markdown","source":["# II. Thực hành"],"metadata":{"id":"-qSljebopi1s"}},{"cell_type":"markdown","source":["Từ bộ dữ liệu iris về thông tin kích thước của các loài hoa với:\n","\n","- X là dữ liệu đầu vào gồm 4 biến: `'sepal length (cm)',\n","  'sepal width (cm)',\n","  'petal length (cm)',\n","  'petal width (cm)'`\n","- y các nhãn tương ứng với: `{0: 'setosa', 1: 'versicolor', 2: 'virginica'}`"],"metadata":{"id":"bYEbW95SpnGi"}},{"cell_type":"markdown","source":["Note: Chuyển bài toán sang phân loại nhị phân có phải loài hoa Setosa hay không? Nhãn 1 là loài hoa Setosa và 0 là không phải."],"metadata":{"id":"EK6wwVwr7s0c"}},{"cell_type":"code","source":["from sklearn.datasets import load_iris\n","data = load_iris()\n","\n","X = data['data']\n","y = data['target']\n","y = np.array([1 if label == 0 else 1 for label in y])\n","print(X.shape, y.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ps2-4-AXp31N","executionInfo":{"status":"ok","timestamp":1675606318668,"user_tz":-420,"elapsed":3,"user":{"displayName":"khanhblog AI","userId":"06481533334230032014"}},"outputId":"aeaa839e-6206-4b67-d700-1bf384f67afd"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["(150, 4) (150,)\n"]}]},{"cell_type":"markdown","source":["1) Thực hiện phân chia tập train/test theo tỷ lệ 80:20."],"metadata":{"id":"gg419ojerx43"}},{"cell_type":"code","source":["# dùng package\n","from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify=y)\n","print(X_train.shape, y_train.shape)\n","print(X_test.shape, y_test.shape)"],"metadata":{"id":"ENTCrv-Or4ca","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675606323173,"user_tz":-420,"elapsed":6,"user":{"displayName":"khanhblog AI","userId":"06481533334230032014"}},"outputId":"4b1feea2-4a00-48bd-c288-ee439b64d530"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["(120, 4) (120,)\n","(30, 4) (30,)\n"]}]},{"cell_type":"markdown","source":["2) Hãy viết hàm Cross Entropy tính toán loss function với đầu vào là nhãn $y$ và dự báo xác suất $\\hat{y}$."],"metadata":{"id":"C-5sguyNpwgU"}},{"cell_type":"code","source":["import numpy as np\n","def cross_entropy(y, y_hat):\n","  '''\n","  Compute cross entropy for binary classification model\n","  Input:\n","    y: label of input observation\n","    y_hat: predicted probability of input observation\n","  Return:\n","    cross entropy value of y and y_hat\n","  '''\n","  cs_loss = -(y*np.log(y_hat)+(1-y)*np.log(1-y_hat))\n","  return cs_loss\n","\n","print(cross_entropy(1, 0.99), cross_entropy(1, 0.01))"],"metadata":{"id":"SxCvzGdhrAsr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675606377777,"user_tz":-420,"elapsed":343,"user":{"displayName":"khanhblog AI","userId":"06481533334230032014"}},"outputId":"0f223c8f-5cb6-4d2c-b9a5-bf55cec1de5c"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["0.01005033585350145 4.605170185988091\n"]}]},{"cell_type":"markdown","source":["3) Hãy viết hàm tính toán giá trị gradient của hàm loss function theo $\\mathbf{w}$"],"metadata":{"id":"QtkbxrUSq9qQ"}},{"cell_type":"code","source":["def sigmoid(x):\n","  z = 1/(1 + np.exp(-x))\n","  return z\n","\n","def gradient_loss(x, y, w):\n","  '''\n","  Input:\n","    x: input feature vector\n","    y: label of observation\n","    w: parameters vector\n","  Return:\n","    compute graidient for Cross Entropy loss function at input (x, y) \n","  '''\n","  y_hat = sigmoid(w.T.dot(x))\n","  gradient = (y_hat - y)*x\n","  return gradient\n","\n","xi = np.array([1, 0.5, 0.25, 0.75, 0.125])\n","yi = 1\n","w = np.random.uniform(low = -0.5, high = 0.5, size = (5,1))\n","gradient_loss(xi, yi, w)"],"metadata":{"id":"Ov9qyYL5rKT4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675606353465,"user_tz":-420,"elapsed":442,"user":{"displayName":"khanhblog AI","userId":"06481533334230032014"}},"outputId":"5ac98c1e-4c56-41e6-a0fd-1b48a06696ca"},"execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([-0.40461051, -0.20230526, -0.10115263, -0.30345788, -0.05057631])"]},"metadata":{},"execution_count":43}]},{"cell_type":"markdown","source":["4) Khởi tạo giá trị ban đầu của véc tơ trọng số $\\mathbf{w}$ và thực hiện huấn luyện mô hình trên tập train theo phương pháp cập nhật gradient descent với learning rate là $0.001$ với 200 vòng lặp."],"metadata":{"id":"mf9EqLDwrLOQ"}},{"cell_type":"code","source":["w = np.random.uniform(low = -0.5, high = 0.5, size = (5))\n","lr = 0.001\n","# Iterating train dataset 200 times:\n","for j in np.arange(0, 200):\n","  # shuffle dataset\n","  np.random.shuffle(X_train)\n","  # Looping overall training dataset:\n","  loss = 0\n","  for i in np.arange(0, X_train.shape[0]):\n","    # with each observation we need to update gradient descent util last observation.\n","    xi = X_train[i, :]\n","    yi = y_train[i]\n","    xi_ext = np.concatenate((np.array([1]), xi), axis=0)\n","    # Compute gradient vector\n","    gradient = gradient_loss(xi_ext, yi, w)\n","    # Update gradient into w\n","    w = w - lr*gradient\n","    y_hat = sigmoid(w.T.dot(xi_ext))\n","    loss_i = cross_entropy(yi, y_hat)\n","    loss += loss_i\n","  print('Iteration {}; Loss function {}; vector w: {}'.format(j, loss, w))\n","  if np.linalg.norm(gradient) <= 0.001:\n","    break"],"metadata":{"id":"AiUrPIFIrlWS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675606397046,"user_tz":-420,"elapsed":1256,"user":{"displayName":"khanhblog AI","userId":"06481533334230032014"}},"outputId":"0db4ad3d-9a10-4689-941b-9c49029e5076"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration 0; Loss function 55.98599065929326; vector w: [-0.02207327  0.21353582  0.26898837 -0.22972019  0.40220503]\n","Iteration 1; Loss function 14.30929199856701; vector w: [-0.0085698   0.29116238  0.30936417 -0.17956224  0.41781551]\n","Iteration 2; Loss function 8.133874907226579; vector w: [-0.00070454  0.33565993  0.33284687 -0.15188649  0.42632924]\n","Iteration 3; Loss function 5.7071277573444466; vector w: [ 0.00486672  0.36693928  0.34950699 -0.13278182  0.43214533]\n","Iteration 4; Loss function 4.405935421886018; vector w: [ 0.00918965  0.39105552  0.36247372 -0.11831167  0.43652515]\n","Iteration 5; Loss function 3.5947009415478983; vector w: [ 0.0127274   0.41068875  0.37308652 -0.10667812  0.44001985]\n","Iteration 6; Loss function 3.039623260653248; vector w: [ 0.01572556  0.42725988  0.38210896 -0.09700175  0.44290502]\n","Iteration 7; Loss function 2.635593032018318; vector w: [ 0.01832936  0.44160904  0.38994209 -0.0886802   0.44537838]\n","Iteration 8; Loss function 2.327924755372516; vector w: [ 0.02063215  0.45427322  0.39687408 -0.08138758  0.44753737]\n","Iteration 9; Loss function 2.0857524050475327; vector w: [ 0.02269733  0.46559362  0.40310203 -0.07493286  0.44943791]\n","Iteration 10; Loss function 1.8903281631571938; vector w: [ 0.02457042  0.4758299   0.40875027 -0.06914234  0.45113654]\n","Iteration 11; Loss function 1.7291772570885435; vector w: [ 0.02628501  0.48518282  0.41392326 -0.06388349  0.45267371]\n","Iteration 12; Loss function 1.5938310074673652; vector w: [ 0.02786628  0.49379252  0.41869553 -0.0590626   0.45408019]\n","Iteration 13; Loss function 1.4785720575765862; vector w: [ 0.02933389  0.50176871  0.4231263  -0.05462339  0.45537066]\n","Iteration 14; Loss function 1.3792102788327285; vector w: [ 0.03070344  0.50920027  0.42726033 -0.05050323  0.45656564]\n","Iteration 15; Loss function 1.2926646686925574; vector w: [ 0.03198752  0.51615585  0.43113858 -0.0466701   0.4576735 ]\n","Iteration 16; Loss function 1.2166041850874292; vector w: [ 0.03319644  0.52269413  0.43479352 -0.04308601  0.45870669]\n","Iteration 17; Loss function 1.1491797717557006; vector w: [ 0.03433871  0.52886595  0.43824801 -0.03971322  0.4596776 ]\n","Iteration 18; Loss function 1.0890015519312473; vector w: [ 0.03542144  0.53470757  0.44152342 -0.03653359  0.46059069]\n","Iteration 19; Loss function 1.0349562822040999; vector w: [ 0.03645068  0.54025462  0.44463742 -0.03352547  0.46145263]\n","Iteration 20; Loss function 0.9861420281193789; vector w: [ 0.0374316   0.54553564  0.44760679 -0.03067208  0.46226857]\n","Iteration 21; Loss function 0.9418254787190175; vector w: [ 0.03836863  0.55057654  0.45044402 -0.02795584  0.46304427]\n","Iteration 22; Loss function 0.9014053509627027; vector w: [ 0.03926561  0.55539669  0.45316077 -0.02536721  0.46378224]\n","Iteration 23; Loss function 0.8643942318271426; vector w: [ 0.04012589  0.56001564  0.45576592 -0.02289273  0.46448656]\n","Iteration 24; Loss function 0.8303720280381245; vector w: [ 0.04095245  0.56444935  0.45827046 -0.020526    0.46515862]\n","Iteration 25; Loss function 0.7989949638979943; vector w: [ 0.04174788  0.56871142  0.4606807  -0.01825736  0.46580168]\n","Iteration 26; Loss function 0.7699598498387037; vector w: [ 0.04251452  0.57281698  0.46300372 -0.01607657  0.46641916]\n","Iteration 27; Loss function 0.7430086817926466; vector w: [ 0.04325441  0.57677522  0.46524654 -0.0139811   0.4670114 ]\n","Iteration 28; Loss function 0.7179278377040084; vector w: [ 0.04396941  0.58059787  0.46741451 -0.01196126  0.46758174]\n","Iteration 29; Loss function 0.6945207570981734; vector w: [ 0.04466117  0.58429394  0.46951227 -0.01001266  0.468131  ]\n","Iteration 30; Loss function 0.6726265418627234; vector w: [ 0.0453312   0.58787146  0.47154395 -0.00813015  0.46866107]\n","Iteration 31; Loss function 0.6521041593914135; vector w: [ 0.04598085  0.59133754  0.47351492 -0.00631195  0.46917224]\n","Iteration 32; Loss function 0.6328271225124955; vector w: [ 0.04661135  0.59469963  0.47542787 -0.00455116  0.46966672]\n","Iteration 33; Loss function 0.6146847805895755; vector w: [ 0.04722382  0.59796302  0.47728653 -0.00284669  0.47014468]\n","Iteration 34; Loss function 0.5975799104724128; vector w: [ 0.04781931  0.60113428  0.47909383 -0.00119307  0.4706079 ]\n","Iteration 35; Loss function 0.581423569451241; vector w: [4.83987346e-02 6.04217544e-01 4.80852482e-01 4.10938217e-04\n"," 4.71056469e-01]\n","Iteration 36; Loss function 0.5661390706186604; vector w: [0.04896297 0.60721909 0.48256539 0.00197017 0.47149222]\n","Iteration 37; Loss function 0.5516558016029955; vector w: [0.04951282 0.61014197 0.48423489 0.0034853  0.47191513]\n","Iteration 38; Loss function 0.5379134418812963; vector w: [0.050049   0.61299074 0.48586301 0.00495929 0.47232611]\n","Iteration 39; Loss function 0.5248546683247316; vector w: [0.0505722  0.61576956 0.4874521  0.00639501 0.47272594]\n","Iteration 40; Loss function 0.5124299195980015; vector w: [0.05108304 0.61848087 0.48900353 0.0077934  0.47311507]\n","Iteration 41; Loss function 0.5005939571605783; vector w: [0.05158212 0.62112878 0.4905195  0.00915685 0.47349411]\n","Iteration 42; Loss function 0.48930644818274055; vector w: [0.05206996 0.62371546 0.49200157 0.01048585 0.47386305]\n","Iteration 43; Loss function 0.47852804388422693; vector w: [0.05254709 0.62624476 0.49345115 0.01178446 0.47422344]\n","Iteration 44; Loss function 0.4682243961681418; vector w: [0.05301397 0.62871837 0.49486973 0.01305216 0.47457485]\n","Iteration 45; Loss function 0.4583669017258154; vector w: [0.05347104 0.63113877 0.49625874 0.01429051 0.47491777]\n","Iteration 46; Loss function 0.4489258605800952; vector w: [0.05391872 0.63350833 0.49761934 0.01550098 0.47525261]\n","Iteration 47; Loss function 0.4398754964055378; vector w: [0.0543574  0.63582942 0.49895288 0.01668479 0.47557982]\n","Iteration 48; Loss function 0.43119157761579996; vector w: [0.05478743 0.63810355 0.50026008 0.01784282 0.47589956]\n","Iteration 49; Loss function 0.42285255870926636; vector w: [0.05520916 0.64033311 0.50154201 0.01897716 0.47621265]\n","Iteration 50; Loss function 0.4148359968520276; vector w: [0.05562292 0.64251998 0.50279981 0.02008885 0.47651916]\n","Iteration 51; Loss function 0.4071249612526996; vector w: [0.056029   0.6446651  0.50403457 0.02117712 0.47681897]\n","Iteration 52; Loss function 0.39970309172655294; vector w: [0.0564277  0.64677037 0.50524706 0.02224356 0.47711252]\n","Iteration 53; Loss function 0.39255345412779236; vector w: [0.05681927 0.64883741 0.50643793 0.02328943 0.47740019]\n","Iteration 54; Loss function 0.38566124899767895; vector w: [0.05720399 0.65086735 0.50760799 0.02431506 0.47768206]\n","Iteration 55; Loss function 0.37901239737563264; vector w: [0.05758208 0.65286193 0.50875818 0.0253217  0.47795849]\n","Iteration 56; Loss function 0.3725942805123228; vector w: [0.05795379 0.65482188 0.50988882 0.02630977 0.47822966]\n","Iteration 57; Loss function 0.3663955219593916; vector w: [0.05831932 0.6567486  0.51100089 0.02727975 0.47849563]\n","Iteration 58; Loss function 0.36040407517083495; vector w: [0.05867889 0.6586434  0.51209488 0.02823283 0.47875683]\n","Iteration 59; Loss function 0.35461018211587286; vector w: [0.05903269 0.66050709 0.51317129 0.02916901 0.47901315]\n","Iteration 60; Loss function 0.34900395432618436; vector w: [0.05938091 0.66234089 0.51423079 0.03008936 0.47926503]\n","Iteration 61; Loss function 0.3435759332160478; vector w: [0.05972372 0.66414578 0.51527391 0.03099423 0.47951248]\n","Iteration 62; Loss function 0.3383179980436445; vector w: [0.06006129 0.66592255 0.51630119 0.03188418 0.4797557 ]\n","Iteration 63; Loss function 0.33322231073327685; vector w: [0.06039379 0.66767209 0.51731311 0.0327596  0.47999484]\n","Iteration 64; Loss function 0.3282816576880439; vector w: [0.06072136 0.66939505 0.51831    0.03362063 0.48022979]\n","Iteration 65; Loss function 0.32348891765784143; vector w: [0.06104416 0.67109256 0.51929263 0.03446809 0.48046093]\n","Iteration 66; Loss function 0.3188374960842485; vector w: [0.06136233 0.67276498 0.52026107 0.03530192 0.48068817]\n","Iteration 67; Loss function 0.3143215304649466; vector w: [0.061676   0.67441331 0.52121604 0.03612281 0.48091171]\n","Iteration 68; Loss function 0.3099344031377599; vector w: [0.0619853  0.67603832 0.52215773 0.03693146 0.48113181]\n","Iteration 69; Loss function 0.30567056758674094; vector w: [0.06229035 0.67764071 0.52308633 0.0377286  0.4813487 ]\n","Iteration 70; Loss function 0.30152536728056384; vector w: [0.06259127 0.67922075 0.52400257 0.03851326 0.48156196]\n","Iteration 71; Loss function 0.2974939312564502; vector w: [0.06288818 0.68077937 0.52490661 0.03928685 0.48177217]\n","Iteration 72; Loss function 0.29357113870190626; vector w: [0.06318117 0.68231706 0.52579881 0.04004929 0.48197919]\n","Iteration 73; Loss function 0.28975280267049813; vector w: [0.06347036 0.68383449 0.52667949 0.04080112 0.48218325]\n","Iteration 74; Loss function 0.2860349334028307; vector w: [0.06375585 0.68533195 0.52754895 0.04154206 0.48238419]\n","Iteration 75; Loss function 0.2824134733706135; vector w: [0.06403773 0.68681037 0.52840753 0.04227316 0.48258238]\n","Iteration 76; Loss function 0.27888426304835096; vector w: [0.06431609 0.68827    0.52925537 0.04299439 0.4827778 ]\n","Iteration 77; Loss function 0.275444387613133; vector w: [0.06459102 0.68971118 0.53009283 0.04370576 0.48297043]\n","Iteration 78; Loss function 0.2720906304038975; vector w: [0.06486261 0.69113437 0.53092016 0.04440749 0.4831603 ]\n","Iteration 79; Loss function 0.26881942154448274; vector w: [0.06513094 0.69254027 0.53173758 0.0451003  0.48334769]\n","Iteration 80; Loss function 0.2656276990077791; vector w: [0.06539609 0.69392911 0.53254538 0.04578407 0.4835325 ]\n","Iteration 81; Loss function 0.2625126071714708; vector w: [0.06565813 0.69530146 0.53334373 0.0464593  0.48371494]\n","Iteration 82; Loss function 0.2594712583680948; vector w: [0.06591714 0.69665763 0.53413287 0.04712597 0.48389497]\n","Iteration 83; Loss function 0.25650144888532983; vector w: [0.06617319 0.69799789 0.53491303 0.04778411 0.48407256]\n","Iteration 84; Loss function 0.25360053432081253; vector w: [0.06642635 0.69932276 0.53568439 0.04843431 0.48424795]\n","Iteration 85; Loss function 0.2507660472150723; vector w: [0.06667668 0.70063246 0.53644718 0.04907645 0.48442107]\n","Iteration 86; Loss function 0.2479957032134771; vector w: [0.06692426 0.70192748 0.53720159 0.049711   0.48459207]\n","Iteration 87; Loss function 0.24528707155872323; vector w: [0.06716913 0.70320829 0.53794784 0.05033829 0.48476104]\n","Iteration 88; Loss function 0.24263798618459798; vector w: [0.06741136 0.70447505 0.53868606 0.05095829 0.48492798]\n","Iteration 89; Loss function 0.2400470481375138; vector w: [0.06765101 0.70572789 0.5394164  0.05157081 0.48509279]\n","Iteration 90; Loss function 0.23751235444059166; vector w: [0.06788813 0.70696725 0.54013912 0.05217617 0.48525558]\n","Iteration 91; Loss function 0.2350318625354145; vector w: [0.06812278 0.70819348 0.54085431 0.05277487 0.48541654]\n","Iteration 92; Loss function 0.23260371257878648; vector w: [0.068355   0.70940682 0.54156214 0.05336679 0.48557558]\n","Iteration 93; Loss function 0.23022641029483157; vector w: [0.06858486 0.71060754 0.54226273 0.05395217 0.4857328 ]\n","Iteration 94; Loss function 0.22789824549661256; vector w: [0.06881239 0.71179594 0.54295632 0.0545312  0.48588826]\n","Iteration 95; Loss function 0.2256177483031948; vector w: [0.06903765 0.71297222 0.54364298 0.05510399 0.48604198]\n","Iteration 96; Loss function 0.2233835156287754; vector w: [0.06926069 0.71413664 0.54432283 0.05567051 0.48619393]\n","Iteration 97; Loss function 0.22119413760394357; vector w: [0.06948154 0.71528946 0.54499614 0.05623103 0.48634422]\n","Iteration 98; Loss function 0.21904825058447974; vector w: [0.06970025 0.71643083 0.54566293 0.05678552 0.48649281]\n","Iteration 99; Loss function 0.21694456285550393; vector w: [0.06991686 0.71756111 0.54632339 0.05733424 0.48663977]\n","Iteration 100; Loss function 0.21488180879921795; vector w: [0.07013142 0.71868043 0.54697758 0.0578773  0.48678514]\n","Iteration 101; Loss function 0.21285873012395856; vector w: [0.07034395 0.71978912 0.54762564 0.05841505 0.48692908]\n","Iteration 102; Loss function 0.21087415699227524; vector w: [0.07055451 0.72088725 0.54826771 0.05894713 0.4870714 ]\n","Iteration 103; Loss function 0.20892715608613438; vector w: [0.07076313 0.72197509 0.54890389 0.05947405 0.48721232]\n","Iteration 104; Loss function 0.20701646462515558; vector w: [0.07096984 0.72305284 0.54953428 0.05999573 0.48735177]\n","Iteration 105; Loss function 0.20514127000465196; vector w: [0.07117468 0.72412059 0.55015893 0.06051223 0.48748977]\n","Iteration 106; Loss function 0.2033006171132747; vector w: [0.07137769 0.72517858 0.55077804 0.06102363 0.48762636]\n","Iteration 107; Loss function 0.2014933991094575; vector w: [0.07157889 0.72622703 0.55139166 0.06153012 0.48776158]\n","Iteration 108; Loss function 0.19971877367653745; vector w: [0.07177833 0.72726607 0.5519999  0.06203175 0.48789544]\n","Iteration 109; Loss function 0.19797584649175254; vector w: [0.07197602 0.72829587 0.55260284 0.06252869 0.48802801]\n","Iteration 110; Loss function 0.19626373418825027; vector w: [0.07217201 0.72931666 0.55320058 0.06302099 0.48815929]\n","Iteration 111; Loss function 0.19458162827529; vector w: [0.07236632 0.73032852 0.55379325 0.06350868 0.48828928]\n","Iteration 112; Loss function 0.19292878028500568; vector w: [0.07255898 0.73133163 0.5543809  0.06399188 0.48841804]\n","Iteration 113; Loss function 0.19130445903945656; vector w: [0.07275002 0.73232607 0.55496365 0.0644705  0.4885455 ]\n","Iteration 114; Loss function 0.18970790991642478; vector w: [0.07293947 0.73331217 0.55554158 0.06494491 0.4886718 ]\n","Iteration 115; Loss function 0.1881382893044779; vector w: [0.07312735 0.73428997 0.55611476 0.06541503 0.48879691]\n","Iteration 116; Loss function 0.18659496374814352; vector w: [0.07331369 0.73525961 0.5566832  0.06588109 0.4889209 ]\n","Iteration 117; Loss function 0.18507737105830813; vector w: [0.07349852 0.73622119 0.55724705 0.06634291 0.4890437 ]\n","Iteration 118; Loss function 0.18358488407581058; vector w: [0.07368186 0.73717489 0.55780635 0.06680071 0.48916539]\n","Iteration 119; Loss function 0.18211676378234962; vector w: [0.07386373 0.73812086 0.55836121 0.06725459 0.48928601]\n","Iteration 120; Loss function 0.18067243051394455; vector w: [0.07404417 0.73905919 0.55891169 0.06770459 0.48940555]\n","Iteration 121; Loss function 0.17925134928848188; vector w: [0.07422319 0.73999001 0.55945787 0.06815075 0.48952404]\n","Iteration 122; Loss function 0.17785292910541683; vector w: [0.07440081 0.74091344 0.55999978 0.06859314 0.48964148]\n","Iteration 123; Loss function 0.17647666916029672; vector w: [0.07457706 0.74182959 0.56053752 0.06903181 0.4897579 ]\n","Iteration 124; Loss function 0.17512203162610718; vector w: [0.07475195 0.74273861 0.56107116 0.06946685 0.48987331]\n","Iteration 125; Loss function 0.17378845596871864; vector w: [0.07492552 0.74364059 0.56160079 0.06989818 0.48998768]\n","Iteration 126; Loss function 0.17247551286501864; vector w: [0.07509778 0.74453563 0.5621264  0.07032608 0.49010111]\n","Iteration 127; Loss function 0.17118267456248767; vector w: [0.07526874 0.74542387 0.56264812 0.0707505  0.49021357]\n","Iteration 128; Loss function 0.16990950984864292; vector w: [0.07543844 0.74630535 0.56316596 0.07117145 0.49032508]\n","Iteration 129; Loss function 0.1686555952830663; vector w: [0.07560689 0.74718022 0.56367998 0.07158902 0.49043565]\n","Iteration 130; Loss function 0.167420392629866; vector w: [0.0757741  0.74804866 0.56419026 0.07200347 0.49054539]\n","Iteration 131; Loss function 0.16620351936997357; vector w: [0.0759401  0.74891061 0.56469682 0.07241451 0.49065415]\n","Iteration 132; Loss function 0.1650046879644331; vector w: [0.0761049  0.74976624 0.56519979 0.07282232 0.49076203]\n","Iteration 133; Loss function 0.1638233796094071; vector w: [0.07626853 0.75061564 0.56569914 0.07322699 0.49086906]\n","Iteration 134; Loss function 0.16265923785376585; vector w: [0.07643099 0.75145893 0.56619498 0.07362856 0.49097522]\n","Iteration 135; Loss function 0.16151187286097426; vector w: [0.07659231 0.75229615 0.56668734 0.07402703 0.49108053]\n","Iteration 136; Loss function 0.16038097113050384; vector w: [0.0767525  0.75312737 0.56717624 0.07442244 0.49118499]\n","Iteration 137; Loss function 0.1592661814213951; vector w: [0.07691157 0.75395272 0.56766174 0.07481487 0.49128864]\n","Iteration 138; Loss function 0.1581671354093916; vector w: [0.07706955 0.75477227 0.56814391 0.07520433 0.49139147]\n","Iteration 139; Loss function 0.1570834617075895; vector w: [0.07722645 0.75558616 0.56862284 0.07559096 0.49149352]\n","Iteration 140; Loss function 0.1560148016374467; vector w: [0.07738228 0.75639441 0.56909848 0.07597479 0.49159481]\n","Iteration 141; Loss function 0.15496092012060705; vector w: [0.07753707 0.7571971  0.56957091 0.07635582 0.49169533]\n","Iteration 142; Loss function 0.1539215025777662; vector w: [0.07769081 0.75799429 0.57004022 0.07673401 0.49179504]\n","Iteration 143; Loss function 0.1528962619417517; vector w: [0.07784353 0.75878612 0.57050642 0.0771095  0.49189404]\n","Iteration 144; Loss function 0.15188483248135137; vector w: [0.07799524 0.75957263 0.57096954 0.07748234 0.4919923 ]\n","Iteration 145; Loss function 0.15088699279386006; vector w: [0.07814596 0.76035388 0.57142963 0.07785252 0.49208983]\n","Iteration 146; Loss function 0.14990249879943182; vector w: [0.07829569 0.7611299  0.57188674 0.07821998 0.49218662]\n","Iteration 147; Loss function 0.14893109535262142; vector w: [0.07844445 0.7619008  0.5723409  0.07858483 0.49228267]\n","Iteration 148; Loss function 0.14797243646688138; vector w: [0.07859226 0.76266672 0.57279216 0.07894724 0.49237807]\n","Iteration 149; Loss function 0.1470263052973969; vector w: [0.07873912 0.76342759 0.57324053 0.07930705 0.49247275]\n","Iteration 150; Loss function 0.14609250175050859; vector w: [0.07888505 0.76418359 0.57368607 0.07966438 0.49256674]\n","Iteration 151; Loss function 0.1451707298531757; vector w: [0.07903007 0.76493471 0.57412882 0.08001928 0.49266007]\n","Iteration 152; Loss function 0.14426074685443926; vector w: [0.07917417 0.76568107 0.57456877 0.08037184 0.49275276]\n","Iteration 153; Loss function 0.14336232929315942; vector w: [0.07931737 0.76642271 0.575006   0.08072207 0.49284482]\n","Iteration 154; Loss function 0.14247527907332463; vector w: [0.0794597  0.76715967 0.57544053 0.08106991 0.49293622]\n","Iteration 155; Loss function 0.141599394008913; vector w: [0.07960114 0.76789203 0.57587242 0.08141542 0.49302699]\n","Iteration 156; Loss function 0.1407344324614261; vector w: [0.07974173 0.76861983 0.57630168 0.0817586  0.49311712]\n","Iteration 157; Loss function 0.1398802201857539; vector w: [0.07988146 0.76934313 0.57672834 0.08209958 0.49320664]\n","Iteration 158; Loss function 0.13903652997976404; vector w: [0.08002034 0.77006201 0.57715244 0.08243832 0.49329556]\n","Iteration 159; Loss function 0.138203150193262; vector w: [0.0801584  0.77077654 0.57757401 0.08277491 0.49338389]\n","Iteration 160; Loss function 0.13737991478900824; vector w: [0.08029564 0.77148671 0.57799307 0.0831093  0.49347161]\n","Iteration 161; Loss function 0.13656664920623007; vector w: [0.08043206 0.77219263 0.57840969 0.08344155 0.49355875]\n","Iteration 162; Loss function 0.13576314120847982; vector w: [0.08056768 0.77289434 0.57882383 0.08377175 0.49364535]\n","Iteration 163; Loss function 0.13496924402963845; vector w: [0.08070251 0.77359185 0.57923555 0.08409978 0.49373133]\n"]}]},{"cell_type":"markdown","source":["5) Dự báo mô hình cho tập test."],"metadata":{"id":"j3wvtppxrkc0"}},{"cell_type":"code","source":["def predict_prob(xi, w):\n","  xi_ext = np.concatenate((np.array([1]), xi), axis=0)\n","  y_hat = sigmoid(w.T.dot(xi_ext))\n","  return y_hat"],"metadata":{"id":"N4AbsoFF6GI2","executionInfo":{"status":"ok","timestamp":1675606418429,"user_tz":-420,"elapsed":417,"user":{"displayName":"khanhblog AI","userId":"06481533334230032014"}}},"execution_count":48,"outputs":[]},{"cell_type":"code","source":["y_test_preds = []\n","for i in np.arange(0, X_test.shape[0]):\n","  xi = X_test[i, :]\n","  y_hat = predict_prob(xi, w)\n","  if y_hat >= 0.5:\n","    y_pred = 1\n","  else:\n","    y_pred = 0\n","  y_test_preds.append(y_pred)"],"metadata":{"id":"UrKYQqVMr8_D","executionInfo":{"status":"ok","timestamp":1675606436577,"user_tz":-420,"elapsed":1,"user":{"displayName":"khanhblog AI","userId":"06481533334230032014"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import classification_report\n","\n","print(classification_report(y_test, y_test_preds))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AWUmG12d7SVr","executionInfo":{"status":"ok","timestamp":1675606519107,"user_tz":-420,"elapsed":2,"user":{"displayName":"khanhblog AI","userId":"06481533334230032014"}},"outputId":"727740be-8525-4a26-ed26-d9cde8d8e0fc"},"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           1       1.00      1.00      1.00        30\n","\n","    accuracy                           1.00        30\n","   macro avg       1.00      1.00      1.00        30\n","weighted avg       1.00      1.00      1.00        30\n","\n"]}]}]}