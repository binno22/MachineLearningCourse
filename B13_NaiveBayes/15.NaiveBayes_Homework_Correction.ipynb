{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"11.NaiveBayes_Homework_Correction.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"7hQ6jFC8Zeyy"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["# I. Lý thuyết (10 câu, mỗi câu 0.5 điểm)\n","\n","1. Trong mô hình xác suất Naive Bayes giả định nào được đặt ra và được xem là ngây ngô (naive)? \n","\n","Đáp án: Mô hình Naive Bayes dựa trên giả định khá ngây ngô về sự độc lập có điều kiện giữa các chiều dữ liệu. Thông thường giả định này khó đạt được trên thực tế. Đặc biệt là đối với dữ liệu dạng văn bản vì các từ trong câu có sự phụ thuộc lẫn nhau.\n","\n","2. Giải thích các đại lượng của công thức Bayes sau\n","\n","$$P(y|x) = \\frac{P(y)P(x|y)}{P(x)}$$"],"metadata":{"id":"ggEqWVZd9Ds-"}},{"cell_type":"markdown","source":["Đáp án:\n","Trong công thức trên các thành phần là:\n","- $P(y|x)$ là _xác suất hậu nghiệm_ (_posteriori probability_): Đây là xác suất chỉ biết được trong điều kiện đã xác định dữ liệu đầu vào $x$. \n","- $P(y)$ là _xác xuất tiên nghiệm_ (_prior probability_): Xác suất dựa trên kinh nghiệm đã có từ trước và hoàn toàn không phụ thuộc vào dữ liệu. Thông thường với kích thước mẫu lớn thì xác suất này được xác định dựa vào tỷ lệ của tấn suất mẫu.\n","- $P(x|y)$ là _hàm hợp lý_ (_likelihood function_). Đây là xác suất của toàn bộ dữ liệu đầu vào $x$ khi đã biến trước $y$.\n","- $P(x)$ là phân phối xác suất của dữ liệu không phụ thuộc vào giá trị của $y$. Xác xuất này được xem như dấu hiệu (_evidence_) của dữ liệu. Xác suất tồn tại như một hằng số nhưng không dễ dàng tính được (_intractable_). Chính vì thế khi so sánh xác suất $P(y|x)$ chúng ta thường tính toán thông qua công thức đồng dạng xác suất:\n","\n","$$P(y|x) \\propto P(y)P(x|y)$$\n","\n","Đồng dạng được hiểu là xác suất ở vế trái bằng vế phải nhân với một hằng số."],"metadata":{"id":"6CapaW7v9LIy"}},{"cell_type":"code","source":[],"metadata":{"id":"BRKqGdTI9D7A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3. Phương pháp MAP tối đa hoá hàm mục tiêu là gì? Ưu điểm của MAP so với MLE là gì?\n","\n","Theo phương pháp $MAP$ ta có \n","\n","$$\\begin{eqnarray}\\hat{\\mathbf{w}} & = & \\arg \\max_{\\mathbf{w}}~ \\log P(\\mathbf{w}| \\mathcal{D}) \\\\\n","& = & \\arg \\max~ \\log \\frac{P(\\mathcal{D}|\\mathbf{w}) P(\\mathbf{w})}{P(\\mathcal{D})} \\\\\n","& = & \\arg \\max~ \\underbrace{\\log P(\\mathcal{D}|\\mathbf{w})}_{\\text{log likelihood}} + \\underbrace{\\log P(\\mathbf{w})}_{\\text{prior}} - \\underbrace{\\log P(\\mathcal{D})}_{\\text{evidence}} \\\\\n","& = & \\arg \\max~ \\log P(\\mathcal{D}|\\mathbf{w}) + \\log P(\\mathbf{w})\n","\\end{eqnarray}$$\n","\n","- Phương pháp MAP sẽ tối đa hoá hàm mục tiêu là xác suất hậu nghiệm $P(\\mathbf{w}| \\mathcal{D})$. Trong đó $\\mathbf{w}$ là véc tơ tham số của mô hình ước lượng (_estimator_) và $\\mathcal{D}$ là bộ dữ liệu đầu vào.\n","- Ưu điểm của phương pháp MAP đó là chúng ta có thể đưa thêm vào niềm tin của mình về mô hình thông qua xác suất tiên nghiệm $P(\\mathbf{w})$ (_prior probability_) để tối đa hoá hàm mục tiêu."],"metadata":{"id":"u-X1KWiV9EI1"}},{"cell_type":"code","source":[],"metadata":{"id":"7ZKQhfWA9Ed0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["4. Nếu $B_1, B_2, ..., B_n$ là một hệ đầy đủ các biến cố và $A$ với $P(A) > 0$ thì với mỗi $k=1, 2, ...n$. Hãy chứng minh \n","$$P(B_k|A) = \\frac{P(B_k)P(A|B_k)}{\\sum_{i=1}^n P(B_i) P(A|B_i)}$$\n","\n","Theo định lý `Bayes` ta có:\n","\n","$$\n","P(A)P(B_k|A) = P(B_k)P(A|B_k)\n","$$ \n","\n","vì cùng bằng $P(AB_k)$.\n","\n","Từ đó suy ra:\n","$$P(B_k|A) = \\frac{P(B_k)P(A|B_k)}{P(A)}$$\n","Mặt khác do $\\{B\\}_{i=1}^n$ là một hệ đầy đủ các biến cố nên suy ra:\n","$$P(A) = \\sum_{i=1}^n P(A, B_i) = \\sum_{i=1}^{n} P(B_i)P(A|B_i)$$\n","Thay vào mẫu ta được công thức cần chứng minh.\n","\n","Lưu ý: Công thức `Bayes` sẽ có dạng:\n","\n","$$P(B_k|A) = \\frac{P(B_k)P(A|B_k)}{P(A)}$$\n","\n","Nếu ta khai triển mẫu của chúng thành tổng thì sẽ thu được dạng mở rộng của công thức `Bayes`:\n","\n","$$P(B_k|A) = \\frac{P(B_k)P(A|B_k)}{\\sum_{i=1}^n P(B_i) P(A|B_i)}$$"],"metadata":{"id":"6pzQVN4R9EtB"}},{"cell_type":"code","source":[],"metadata":{"id":"T8ZOwp4J9E11"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Một người đi tới công ty làm việc sử dụng 30% lượt để đi ô tô, 30% lượt để đi bộ, và 40% lượt để đi xe bus. Biết rằng, người đó sẽ bị trễ 3% khi lái xe ô tô, 10% số lượt khi đi bộ, và 7% khi đi xe bus. \n","5. Tính xác xuất để người đó đi muộn?\n","6. Xác suất người đó đi xe bus nếu anh ta đến muộn là bao nhiêu?\n","7. Tính xác suất để người đó đi bộ nếu đến đúng giờ."],"metadata":{"id":"qJJBZhBG9E_F"}},{"cell_type":"markdown","source":["- Gọi $H$ là biến cố người đó đi làm muộn, thì $\\bar{H}$ là biến cố người đó đi làm đúng giờ \n","\n","- Gọi $A, B, C$ lần lượt là biến cố người đó đi làm bằng ô tô, đi bộ và đi xe bus. Thì ta có \n","$$P(A) = 0.3, P(B) = 0.3, P(C) = 0.4$$\n","\n","Mà người đó sẽ bị trễ 10% số lượt khi đi bộ, 3% số lượt khi lái xe ô tô và 7% số lượt khi đi xe bus nên ta có \n","$$\n","P(H|A) = 0.03 \\\\\n","P(H|B) = 0.1 \\\\\n","P(H|C) = 0.07 \\\\\n","$$\n","Từ đó:\n","\n","5. Xác suất để người đó đi làm muộn là \n","$$\\begin{eqnarray}P(H) & = & P(A)P(H|A) + P(B)P(H|B) + P(C)P(H|C) \\\\\n","& = & 0.3*0.03 + 0.3*0.1 + 0.4 * 0.07 \\\\\n","& = & 0.067\n","\\end{eqnarray}$$\n","\n","6. Xác suất người đó đi xe buýt nếu anh ta đến muộn là\n","$$P(C|H) = \\frac{P(C) P(H|C)}{P(H)} = \\frac{0.4 \\times 0.07 }{0.067} = 0.418$$\n","\n","7. Ta có \n","$$P(\\bar{H}) = 1 - P(H) = 0.933$$ \n","\n","Xác suất để người đó đến đúng giờ khi đi bộ\n","$$P(B|\\bar{H}) = \\frac{P(B)P(\\bar{H}|B)}{P(\\bar{H})} = \\frac{0.3 \\times 0.9}{0.933} = 0.289$$"],"metadata":{"id":"W5VhQBYrIazw"}},{"cell_type":"code","source":[],"metadata":{"id":"QEAjdt5p9FHO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["8. Xác suất để một người bị bệnh lao là 0.05%, và xác suất để dương tính là với 99% độ chính xác. Tính xác suất để một người bị bệnh lao nếu người đó khi test bị dương tính. "],"metadata":{"id":"T-o8bNQIq8xh"}},{"cell_type":"markdown","source":["- Gọi $H$ là biến cố người đó dương tính với bệnh lao\n","- Gọi $A, B$ là lần lượt là biến cố bị bệnh và không bị bệnh  \n","$$P(A) = 0.0005, P(B) = 0.9995$$\n","$$P(H|A) = 0.99 \\\\\n","P(H|B) = 0.01\n","$$\n","Nên xác suất để một người dương tính với bệnh lao là \n","$$P(H) = P(A)P(H|A) + P(B)P(H|B) = 0.0005 \\times 0.99  + 0.9995 \\times 0.01 = 0.01049$$\n","\n","Xác suất để một người bị bệnh lao nếu người đó khi test bị dương tính\n","$$P(A|H) = \\frac{P(A)P(H|A)}{P(H)} = \\frac{0.0005 \\times 0.99 }{0.01049} = 0.0472$$ "],"metadata":{"id":"osSPlfXXq_mv"}},{"cell_type":"code","source":[],"metadata":{"id":"cjkISrM1HnQG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Giả sử $Y_1, Y_2, ..., Y_n$ là các quan sát độc lập và $Y_i \\sim N(\\beta x_i, \\sigma^2)$, trong đó $x_i, \\sigma$ là các hằng số biết trước, $\\beta$ là tham số chưa biết. Nhưng biết xác suất tiên nghiệm $\\beta \\sim N(\\beta_0, \\eta^2)$ và $\\beta_0, \\eta^2$ là các hằng số biết trước. \n","9. Tìm phân bố xác suất hậu nghiệm của $\\beta$ khi biết trước $y$\n","10. Tìm giới hạn của phân bố xác suất hậu nghiệm khi $\\eta^2 \\rightarrow \\infty$"],"metadata":{"id":"xS2iz7e7ZiiQ"}},{"cell_type":"markdown","source":["9. Ta có $Y_i \\sim N(\\beta x_i, \\sigma^2)$ nên \n","$$f(y_1, y_2,...y_n|\\beta) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi \\sigma^2}} e^{-\\frac{1}{2\\sigma^2}(y_i - \\beta x_i)^2} \\propto e^{-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n}(y_i - \\beta x_i)^2} $$\n","\n","Lại có, xác suất tiên nghiệm \n","$$P(\\beta) = \\frac{1}{\\sqrt{2\\pi \\eta^2}} e^{-\\frac{1}{2\\eta^2}(\\beta - \\beta_0)^2}$$\n","\n","Mà công thức xác suất hậu nghiệm \n","$$P(\\beta|y) \\propto P(\\beta) P(y|\\beta) \\propto e^{-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n}(y_i - \\beta x_i)^2 - \\frac{1}{2\\eta^2}(\\beta - \\beta_0)^2 } = e^{- \\frac12 \\left( \\frac{1}{\\sigma^2} \\sum_{i=1}^{n}(y_i - \\beta x_i)^2 + \\frac{1}{\\eta^2}(\\beta - \\beta_0)^2 \\right)} = e^{-\\frac12 M}$$\n","\n","Xét \n","\\begin{align*}\n","M &= \\frac{1}{\\sigma^2} \\sum_{i=1}^{n}(y_i - \\beta x_i)^2 + \\frac{1}{\\eta^2}(\\beta - \\beta_0)^2 \\\\\n","&= \\frac{\\sum y_i^2}{\\sigma^2} - 2\\beta \\frac{\\sum y_i x_i}{\\sigma^2} + \\beta^2\\frac{\\sum x_i^2}{\\sigma^2} + \\beta^2 \\frac{1}{\\eta^2} - 2\\beta \\frac{\\beta_0}{\\eta^2} + \\frac{\\beta_0^2}{\\eta^2} \\\\\n","&= \\beta^2 \\underbrace{\\left( \\frac{\\sum x_i^2}{\\sigma^2} + \\frac{1}{\\eta^2} \\right)}_{1/\\sigma_1^2} - 2\\beta \\underbrace{\\left(\\frac{\\sum y_i x_i}{\\sigma^2} + \\frac{\\beta_0}{\\eta^2} \\right)}_{\\beta_1/\\sigma_1^2} + \\frac{\\sum y_i^2}{\\sigma^2} +  \\frac{\\beta_0^2}{\\eta^2} \\\\\n","&= \\beta^2 \\frac{1}{\\sigma_1^2} - 2\\beta \\frac{\\beta_1}{\\sigma_1^2} + \\beta_1^2\\frac{1}{\\sigma_1^2} + \\underbrace{\\frac{\\sum y_i^2}{\\sigma^2} +  \\frac{\\beta_0^2}{\\eta^2} - \\beta_1^2\\frac{1}{\\sigma_1^2}}_{C}\\\\\n","&= \\frac{(\\beta-\\beta_1)^2}{\\sigma_1^2} + C\n","\\end{align*}\n","Trong đó \n","$$\\sigma_1^2 = \\frac{1}{\\frac{\\sum x_i^2}{\\sigma^2} + \\frac{1}{\\eta^2}}, \\beta_1 = \\sigma_1^2 \\frac{\\sum y_i x_i}{\\sigma^2} + \\frac{\\beta_0}{\\eta^2}$$\n","Như vậy phân phối xác suất hậu nghiệm $\\beta|y \\propto N(\\beta_1, \\sigma_1^2)$\n","\n","\n","10. Ta có $\\eta^2 \\rightarrow \\infty$ thì:\n","$$\\sigma_1^2 \\rightarrow \\frac{1}{\\frac{\\sum_{x_i^2}}{\\sigma^2}} = \\frac{\\sigma^2}{\\sum x_i^2}, \\\\ \\beta_1 \\rightarrow \\frac{\\sigma^2}{\\sum x_i^2} \\frac{\\sum{y_ix_i}}{\\sigma^2}= \\frac{\\sum y_i x_i}{\\sum x_i^2}$$ \n","\n","(theo cách đặt ở cuối câu 9). Do đó \n","\n","$$\\beta | y \\sim N(\\frac{\\sum y_i x_i}{\\sum x_i^2}, \\frac{\\sigma^2}{\\sum x_i^2})$$"],"metadata":{"id":"xZiyLQ0eHoTb"}},{"cell_type":"code","source":[],"metadata":{"id":"tBVTgqjsjbMK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["II. Thực hành (5 câu, mỗi câu 1 điểm)\n","\n","1. Cho bộ dữ liệu về phân loại văn bản gồm các câu theo chủ để như sau:\n","\n","**ẩm thực**:\n","```\n","- phở là một trong những món ăn truyền thống của Hà Nội\n","- hương vị của vịt quay tạo nên một vị phở đậm đà\n","```\n","\n","**thể thao**:\n","```\n","- đội tuyển bóng đá nam đã tạo ra kì tích tại đại hội thể thao Seagame 31.\n","- world cup là mùa giải bóng đá lớn nhất hành tinh.\n","```\n","\n","Hãy xây dựng mô hình `Naive Bayes` với phân phối Multinomial để phân loại các câu văn theo topic mà không sử dụng thư viện scikit-learn.\n","\n"],"metadata":{"id":"xxYDsO7Vzuiy"}},{"cell_type":"markdown","source":["Gọi $x_i$ là từ thứ $i$ trong bộ từ điển có kích thước là $d$, $\\mathbf{x}_j$ là văn bản thứ $j$. Tần suất của từ $x_i$ xuất hiện trong văn bản thứ $\\mathbf{x}_j$ là $N_{ij}$. Theo công thức Bayes:\n","\n","$$\\begin{eqnarray}P(y=c|\\mathbf{x}_j) & = & \\frac{P(\\mathbf{x}_j | y=c) P(y=c)}{P(\\mathbf{x}_j)} \\\\\n","& \\propto & \\underbrace{P(y=c)}_{\\text{prior}} \\underbrace{\\prod_{i=1}^{d} P(x_i| y=c)^{N_{ij}}}_{\\text{likelihood}}\n","\\end{eqnarray}$$\n","\n","Ở đây ta có thể xem tập hợp $(N_{1j}, N_{2j}, \\dots, N_{dj} )$ như là véc tơ phân phối tần suất của toàn bộ các từ trong từ điển trong văn bản $\\mathbf{x}_j$\n","\n","Lấy log 2 vế ta có:\n","$$\\log P(y=c|\\mathbf{x}_j) \\propto \\log P(y=c) + N_{ij}\\sum_{i=1}^{d} \\log P(x_i| y=c)$$"],"metadata":{"id":"nf8HFD2CmY9u"}},{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"EE68y4encrfC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentence1 = 'phở là một trong những món ăn truyền thống của Hà Nội'\n","sentence2 = 'hương vị của vịt quay tạo nên một vị phở đậm đà'\n","sentence3 = 'đội tuyển bóng đá nam đã tạo ra kì tích tại đại hội thể thao Seagame 31'\n","sentence4 = 'world cup là mùa giải bóng đá lớn nhất hành tinh'\n","\n","X_train = [sentence1, sentence2, sentence3, sentence4]\n","y_train = [0, 0, 1, 1]"],"metadata":{"id":"euCM4fotNx0y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def process(sentence):\n","    '''\n","    split a sentence into a list\n","    '''\n","    wordList = sentence.split(' ')\n","    return wordList\n","\n","\n","def getFreqs(data, label):\n","    '''\n","    count frequency of each word in a label\n","    '''\n","    freqs = {}\n","    for sentence, label in zip(data, label):\n","        for word in process(sentence):\n","            # define key of dict\n","            pair = (word, label)\n","            # If the key exists in the dictionary, increment to 1\n","            if pair in freqs:\n","                freqs[pair] += 1        \n","            # else, if the key new, set equal to 1\n","            else:\n","                freqs[pair] = 1\n","    \n","    return freqs\n","\n","\n","def getNumberWords(freqs, label):\n","    '''\n","    count number of word for each label\n","    '''\n","    N = 0\n","    for pair, freq in freqs.items():\n","        if pair[1] == label:\n","            N += freq\n","    return N\n","\n","\n","def likelihood(Nci, Nc, d, alpha=1):\n","    '''\n","    Laplace smoothing formula to compute probability of word x_i belong to class c\n","    Input:\n","      Nc: a number of word in all documents of class c\n","      Nci: a number of word i in document i belong to class c\n","    Output:\n","      Laplace smoothing probability of word i in class c\n","    '''\n","    lmbda = (Nci + alpha) / (Nc + d * alpha)\n","    return lmbda \n","\n","\n","def lookup(freqs, word, label):\n","    \"\"\"\n","    Input:\n","        freqs: a dictionary with the frequency of each pair (or tuple)\n","        word: the word to look up\n","        label: the label corresponding to the word\n","    Output:\n","        n: the number of times the word with its corresponding label appears.\n","    \"\"\"\n","    n = 0  # freqs.get((word, label), 0)\n","    pair = (word, label)\n","    if (pair in freqs):\n","        n = freqs[pair]\n","    return n\n","\n","\n","def train(X_train, y_train):\n","    \n","    # calculate p(c)\n","    nSample = len(y_train)\n","    labelList = np.unique(y_train)\n","    priorDict = {label: np.sum(y_train == label) / nSample for label in labelList}\n","    logprior = {label: np.log(prior) for label, prior in priorDict.items()}\n","    \n","    \n","    # calculate likelihood\n","    loglikelihood = {}\n","    freqs = getFreqs(X_train, y_train)\n","    vocab = set([pair[0] for pair in freqs.keys()])\n","    V = len(vocab)\n","    for label in labelList:\n","        # Total words of label c\n","        Nc = getNumberWords(freqs, label)\n","        for word in vocab:\n","            # total word x_i in label c\n","            Nic = lookup(freqs, word, label)\n","            # likelihood P(xi|y=c)^Nic\n","            likeli = Nic*np.log(likelihood(Nic, Nc, V))\n","            loglikelihood[(word, label)] = likeli\n","            \n","    return logprior, loglikelihood, vocab, freqs, labelList\n","\n","\n","# Train data\n","logprior, loglikelihood, vocab, freqs, labels = train(X_train, y_train)"],"metadata":{"id":"LemXLa0Ncqss"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2. Sử dụng mô hình vừa huấn luyện, hãy dự báo nhãn cho các văn bản sau:\n","\n","```\n","các món ăn đường phố là một văn hóa truyền thống của người Việt.\n","```\n","và \n","\n","```\n","Quang Hải đã được mua lại bởi một đội bóng truyền thống tại Pháp.\n","```"],"metadata":{"id":"FS8kK6j6E1O3"}},{"cell_type":"code","source":["def predict(sentence, logprior, loglikelihood, vocab, freqs, labels):\n","    wordList = process(sentence)\n","    wordSet = set(wordList)\n","    \n","    scoreList = []\n","    V = len(vocab)\n","    for label in labels:\n","        p = 0\n","        pc = logprior[label]\n","        p += pc\n","        for word in wordSet:\n","            if word in vocab:\n","                loglikeli = loglikelihood[(word, label)]\n","            else: \n","                Nic = 0\n","                Nc = getNumberWords(freqs, label)\n","                loglikeli = np.log(likelihood(Nic, Nc, V))\n","                \n","            p += loglikeli\n","        scoreList.append(p)\n","        \n","    return np.argmax(scoreList)\n","\n","\n","X_test = ['các món món ăn đường phố là một văn hóa truyền thống của người Việt', \n","          'Quang Hải đã được mua lại bởi một đội bóng truyền thống tại Pháp']\n","\n","for sentence in X_test:\n","    y_pred = predict(sentence, logprior, loglikelihood, vocab, freqs, labels)\n","    print(f'Sentence: {sentence} - Label: {y_pred}')"],"metadata":{"id":"nOsk8STiE0oN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661438342232,"user_tz":-420,"elapsed":536,"user":{"displayName":"khanhblog AI","userId":"06481533334230032014"}},"outputId":"fc329934-aef5-4648-b833-a3614a153d3a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sentence: các món món ăn đường phố là một văn hóa truyền thống của người Việt - Label: 1\n","Sentence: Quang Hải đã được mua lại bởi một đội bóng truyền thống tại Pháp - Label: 0\n"]}]},{"cell_type":"markdown","source":["3. Xây dựng mô hình `NaiveBayes Classifier` với phân phối Gaussian để phân loại các loài hoa từ bộ dữ liệu iris mà không cần sử dụng scikit-learn. Để load bộ dữ liệu iris có thể sử dụng sklearn như sau:\n","\n","```\n","from sklearn import datasets\n","\n","iris = datasets.load_iris()\n","X = iris.data\n","y = iris.target\n","```\n"],"metadata":{"id":"zLMDXc426uHD"}},{"cell_type":"code","source":["from sklearn import datasets\n","\n","iris = datasets.load_iris()\n","X = iris.data\n","y = iris.target"],"metadata":{"id":"w5rTZiKp6vtA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# we have to compute mean/standard deviation belong to each class\n","# based on this to compute likelihood P(x|y=c) = f(x; mu_c, std_c)\n","meanList = []\n","stdList = []\n","\n","for label in np.unique(y):\n","    indices = np.where(y == label)[0]\n","    meanList.append(X[indices, :].mean(axis=0))\n","    stdList.append(X[indices, :].std(axis=0))\n","\n","meanList, stdList"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"193hj0oLBrkP","executionInfo":{"status":"ok","timestamp":1661438352692,"user_tz":-420,"elapsed":416,"user":{"displayName":"khanhblog AI","userId":"06481533334230032014"}},"outputId":"253c445e-40b3-459b-bd21-e87f636f7a9c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["([array([5.006, 3.428, 1.462, 0.246]),\n","  array([5.936, 2.77 , 4.26 , 1.326]),\n","  array([6.588, 2.974, 5.552, 2.026])],\n"," [array([0.34894699, 0.37525458, 0.17191859, 0.10432641]),\n","  array([0.51098337, 0.31064449, 0.46518813, 0.19576517]),\n","  array([0.62948868, 0.31925538, 0.54634787, 0.27188968])])"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["\n","4. Dự báo đối với hai quan sát mới có giá trị \n","`['sepal length (cm)',\n","  'sepal width (cm)',\n","  'petal length (cm)',\n","  'petal width (cm)']` lần lượt như sau:\n","\n","```\n","[6.4, 3.1, 5.5, 1.8]\n","và  \n","[4.9, 3. , 1.4, 0.2],\n","```"],"metadata":{"id":"jwcJnXMFcouN"}},{"cell_type":"code","source":["# 4\n","from scipy import stats\n","\n","def predict(x, meanList, stdList):\n","    probabilities = []\n","    classes = len(meanList)\n","    for c in range(classes):\n","        score = stats.multivariate_normal.pdf(x, mean=meanList[c], cov=stdList[c])\n","        probabilities.append(score)\n","    return np.argmax(probabilities)\n","\n","x1 = [6.4, 3.1, 5.5, 1.8]\n","x2 = [4.9, 3. , 1.4, 0.2]\n","\n","print(predict(x1, meanList, stdList))\n","print(predict(x2, meanList, stdList))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BI_2wWOuD6Tq","executionInfo":{"status":"ok","timestamp":1661438355268,"user_tz":-420,"elapsed":493,"user":{"displayName":"khanhblog AI","userId":"06481533334230032014"}},"outputId":"290f86d6-319c-492c-fa36-1c1894b73dc5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2\n","0\n"]}]},{"cell_type":"markdown","source":["5. Thực hành xây dựng lại mô hình `NaiveBayes` theo package `scikit-learn` để phân loại các loài hoa trong bộ dữ liệu iris."],"metadata":{"id":"-J04nOnD6wj_"}},{"cell_type":"code","source":["from sklearn.naive_bayes import GaussianNB\n","\n","clf = GaussianNB()\n","clf.fit(X, y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WRVfeFcFDlJg","executionInfo":{"status":"ok","timestamp":1661086647589,"user_tz":-420,"elapsed":13,"user":{"displayName":"Đặng Quý Anh","userId":"05985986265933843899"}},"outputId":"fdeb23eb-896f-47d2-892b-3d5c4f7b3d7f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["GaussianNB()"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["X_test = [x1, x2]\n","clf.predict(X_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TV0tejkXFsL4","executionInfo":{"status":"ok","timestamp":1661086647590,"user_tz":-420,"elapsed":11,"user":{"displayName":"Đặng Quý Anh","userId":"05985986265933843899"}},"outputId":"84bc40e2-a401-4966-d4fa-170efe67e608"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([2, 0])"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":[],"metadata":{"id":"9fB7JfyLdGNx"},"execution_count":null,"outputs":[]}]}