{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNj0WtkDb_tw"
      },
      "source": [
        "# I. Thực hành"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Câu 1: Trong phương pháp bag-of-word thì mỗi một đoạn văn bản sẽ được biến đổi \n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n",
        "thành véc tơ đặc trưng như thế nào?\n",
        "\n",
        "\n",
        "(A). Thành véc tơ tần suất của các từ xuất hiện trong từ điển. \n",
        "\n",
        "B. Thành véc tơ one-hot, từ nào xuất hiện trong văn bản thì có giá trị 1, trái lại nhận giá trị 0.\n",
        "\n",
        "C. Thành véc tơ tf-idf của từ.\n",
        "\n",
        "D. Thành một véc tơ trong không gian latent space."
      ],
      "metadata": {
        "id": "V71cW0PFNWoO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bFR4B-1dv0_"
      },
      "source": [
        "Câu 2: Hai phương pháp `bigram` và `trigram` trong `bag-of-n-gram` sẽ mã hoá một văn bản như thế nào?\n",
        "\n",
        "A. `Bigram` sẽ ghép ba từ liên tiếp thành một token. `Trigram` sẽ ghép hai từ liên tiếp nhau thành một token.\n",
        "\n",
        "(B). `Bigram` sẽ ghép hai từ liên tiếp thành một token. `Trigram` sẽ ghép ba từ liên tiếp nhau thành một token.\n",
        "\n",
        "C. `Bigram` sẽ chia mỗi một từ thành một token. `Trigram` sẽ ghép ba từ liên tiếp nhau thành một token.\n",
        "\n",
        "D. `Bigram` sẽ ghép hai từ bất kì trong khoảng cách gần thành một token. `Trigram` sẽ ghép ba từ liên tiếp trong khoảng cách gần thành một token."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uqu9GnNDfG_0"
      },
      "source": [
        "Câu 3: Giải thích ý nghĩa của chỉ số tf-idf được sử dụng để mã hoá các từ trong bộ văn bản. Một từ có tf-idf cao thì chứng tỏ điều gì?\n",
        "\n",
        "A. `tf-idf` là chỉ số mã hóa dựa trên độ dài của văn bản. Khi `tf-idf` cao chứng tỏ mức độ phổ biến của từ càng cao và ngược lại.\n",
        "\n",
        "B. `tf-idf` là chỉ số mã hóa dựa trên tần suất. Khi `tf-idf` cao chứng tỏ mức độ phổ biến của từ càng thấp và ngược lại.\n",
        "\n",
        "C. `tf-idf` nhằm đánh giá mức độ phổ biến của một từ dựa vào tần suất xuất hiện của từ trong một văn bản và tần suất các văn bản xuất hiện từ đó. Khi `tf-idf` cao chứng tỏ mức độ phổ biến của từ càng cao và ngược lại.\n",
        "\n",
        "(D). `tf-idf` nhằm đánh giá mức độ quan trọng của một từ dựa vào tần suất xuất hiện của từ trong một văn bản và tần suất các văn bản xuất hiện từ đó. Khi `tf-idf` cao chứng tỏ mức độ quan trọng của từ càng cao và ngược lại."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Câu 4: Kiến trúc nào thường được sử dụng để trích xuất đặc trưng (`Feature Extractor`) trong các bộ dữ liệu ảnh:\n",
        "\n",
        "A. Thuật toán SVM\n",
        " \n",
        "B. Thuật toán MLP\n",
        "\n",
        "(C). Thuật toán CNN\n",
        "\n",
        "D. Thuật toán LSTM."
      ],
      "metadata": {
        "id": "qHA9G2YlNcnG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Câu 5: Tại sao chúng ta cần thực hiện các kĩ thuật chuẩn hóa dữ liệu như `Min-max Scaling`, `Standardization Scaling`, hoặc `Unit length Scaling` trên dữ liệu $\\mathbf{X}$ trước khi huấn luyện mô hình:\n",
        "\n",
        "A. Để đồng nhất đơn vị của các biến trước khi huấn luyện mô hình.\n",
        "\n",
        "B. Giúp các thuật toán ổn định hơn.\n",
        "\n",
        "C. Giảm chi phí tính toán.\n",
        "\n",
        "(D). Cả ba đáp án trên."
      ],
      "metadata": {
        "id": "BA3ZjsDxPR8T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# II. Thực hành"
      ],
      "metadata": {
        "id": "PbcBUzP2NN7t"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cx9tmI6Rh9ow"
      },
      "source": [
        "Câu 6: Thực hành phân loại văn bản dựa trên phương pháp bag-of-word và tf-idf đối với bộ dữ liệu [10Topics](https://github.com/duyvuleo/VNTC/tree/master/Data/10Topics/Ver1.1).\n",
        "\n",
        "Gợi ý: Bạn cần thực hiện theo các bước sau đây:\n",
        "\n",
        "- Bước 1: Download dữ liệu.\n",
        "\n",
        "- Bước 2: Tiền xử lý dữ liệu (`Data pre-processing`). Sử dụng phương pháp `bag-of-word` để mã hóa văn bản thành véc tơ.\n",
        "\n",
        "- Bước 3: Phân chia tập `train/validation` và huấn luyện mô hình."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OmlqOQsgBLj"
      },
      "source": [
        "**Download Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ut2qRW08IxhL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa67a392-8d88-4eff-b0e9-9ed93122b540"
      },
      "source": [
        "# Google drive mount point\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHwSQ74qItNe"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/duyvuleo/VNTC/master/Data/10Topics/Ver1.1/Test_Full.rar \\\n",
        "&& unrar x Test_Full.rar \\\n",
        "&& rm -f Test_Full.rar && ls Test_Full"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/duyvuleo/VNTC/master/Data/10Topics/Ver1.1/Train_Full.rar \\\n",
        "&& unrar x Train_Full.rar \\\n",
        "&& rm -f Train_Full.rar && ls Train_Full"
      ],
      "metadata": {
        "id": "KIsjWuf-GPG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eg8fYSRLKjgn"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.impute import KNNImputer, SimpleImputer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import fbeta_score\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data preprocessing**"
      ],
      "metadata": {
        "id": "QCsDLGvoRpww"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trong quá trình lấy data em chia data theo từng chuyên đề"
      ],
      "metadata": {
        "id": "5jiE15VDo-F-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "Full_texts = {}\n",
        "for root, dirs, files in os.walk(\"/content/Train_Full\", topdown=True):\n",
        "  for dir in dirs:\n",
        "    Full_texts[str(dir)]=[]\n",
        "  for file in files:\n",
        "    #Full_texts[str(root[20:])]\n",
        "    file_path = root +'/'+ file\n",
        "    sentences = open(file_path,'r',encoding = 'utf-16').readlines()\n",
        "    for sentence in sentences:\n",
        "      Full_texts[str(root[20:])].append(sentence.replace(\"\\n\",\"\"))"
      ],
      "metadata": {
        "id": "NDrYYpU1J7Mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "string= Full_texts['Doi song'][0:1]\n",
        "vec=CountVectorizer(tokenizer=lambda txt: txt.split())\n",
        "X= vec.fit_transform(string)\n",
        "vec.get_feature_names()\n",
        "X.toarray()"
      ],
      "metadata": {
        "id": "wkY1xKCGiRkk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50204dec-05bd-4d8a-b5f3-82296a7daff5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "for texts in Full_texts:\n",
        "   vec=CountVectorizer(tokenizer=lambda txt: txt.split())\n",
        "   X= vec.fit_transform(Full_texts[texts])\n",
        "X.toarray()"
      ],
      "metadata": {
        "id": "x6tLwvYAgDed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "324ab66f-45ed-49a8-a14c-de82931b39d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(max_df = 0.9)\n",
        "for texts in Full_texts:\n",
        "  X = vectorizer.fit_transform(Full_texts[texts])\n"
      ],
      "metadata": {
        "id": "ZpOUOvXsmZQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAvsdde0nV9B",
        "outputId": "74e6aea0-02c3-4a06-aa67-648928907b04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(26889, 16170)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MN1w4FRbgGct"
      },
      "source": [
        "**Model training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jm9A9XGGlG8y"
      },
      "source": [
        "Câu 7: Thực hành biến đổi dữ liệu với python\n",
        "\n",
        "- Lấy ra current date và current time\n",
        "- Current year\n",
        "- Month of year\n",
        "- Week number of the year\n",
        "- Weekday of the week\n",
        "- Day of year\n",
        "- Day of the month\n",
        "- Day of week\n",
        "\n",
        "\n",
        "Gợi ý: Sử dụng package datetime."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "def parser(x):\n",
        "    # Để biết được định dạng strftime của một chuỗi kí tự ta phải tra trong bàng string format time\n",
        "    return datetime.strptime(x, '%Y/%m/%d %H:%M:%S')\n",
        "\n",
        "for texts in Full_texts:\n",
        "  check= {}\n",
        "  check.append(texts : Full_texts[texts])\n",
        "  dataset=pd.DataFrame(check)\n",
        " # dataset[check] = dataset[check].map(lambda x: parser(x))\n",
        "  print(check)\n",
        "  break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "vMmXukZ2nq8W",
        "outputId": "6a0039c5-8696-46b3-e7f2-dab67ff7c1ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-45-766471380e36>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    check.append(texts : Full_texts[texts])\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for texts in Full_texts:\n",
        " dataset=pd.DataFrame(Full_texts[texts])\n",
        " print (type(Full_texts[texts]))\n",
        "\n",
        " dict(zip('',''))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZkr48vhtVs3",
        "outputId": "8f7aa0bf-2d02-46ce-928d-fffbe00d9b0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFSYMMKuqRiC"
      },
      "source": [
        "Câu 8: Sử dụng AutoEncoder để giảm chiều bộ dữ liệu sau từ 50 chiều về 10 chiều.\n",
        "Code khởi tạo dữ liệu:\n",
        "\n",
        "```\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Khởi tạo dữ liệu example\n",
        "X, y = make_classification(n_samples=500, n_features=50, random_state=123)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Câu 9: Xây dựng mô hình phân loại trên 10 biến trong không gian giảm chiều và đánh giá độ chính xác mô hình."
      ],
      "metadata": {
        "id": "-8kTPQszSfoe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQWhaLPWqnHR"
      },
      "source": [
        "Câu 10: Sử dụng các kĩ thuật lựa chọn đặc trưng khác nhau để lựa chọn ra 10 biến đầu vào từ 50 biến đầu vào gốc. Xây dựng mô hình phân loại trên các biến được lựa chọn và đánh giá độ chính xác."
      ]
    }
  ]
}