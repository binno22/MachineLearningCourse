{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SR4mkBllVIIw"
   },
   "source": [
    "# I. Lý thuyết (10 câu, 0.5 điểm/câu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0nTCKfnmVP_6"
   },
   "source": [
    "1) Phát biểu nào sau đây là đúng về kiến trúc MLP\n",
    "\n",
    "A. Là kiến trúc gồm nhiều Layers liên tiếp xử lý dữ liệu theo thứ tự.\n",
    "\n",
    "B. Mỗi một layer của MLP sẽ bao gồm nhiều units, mỗi unit đóng vai trò tương tự như một biến.\n",
    "\n",
    "C. Thứ tự các layers của MLP là `input layer --> các hidden layers --> output layer`\n",
    "\n",
    "D. Cả ba đáp án trên."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K21piLx35sf1"
   },
   "source": [
    "1-D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A42FkbHNWC6J"
   },
   "source": [
    "2) Các đặc trưng được tạo ra từ MLP có gì khác biệt so với các thuật toán machine learning truyền thống ?\n",
    "\n",
    "A. Là những đặc trưng được xác định từ trước thông qua feature engineering từ người xây dựng mô hình.\n",
    "\n",
    "B. Là những đặc trưng được tạo thành từ Polynormial Feature.\n",
    "\n",
    "C. Là những đặc trưng đã được chuẩn hóa theo Min-max scaling.\n",
    "\n",
    "D. Là những đặc trưng ẩn (_latent features_) được học thông qua quá trình huấn luyện mô hình."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WD9jU5s77Zsy"
   },
   "source": [
    "2-D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AxRBoeqtWBky"
   },
   "source": [
    "3) Vì sao khi tính toán một unit tại layer tiếp theo thì cần áp dụng activation function lên tổ hợp tuyến tính của các units đầu vào từ layer ngay liền trước?\n",
    "\n",
    "A. Để giảm thiểu chi phí tính toán.\n",
    "\n",
    "B. Để khắc phục hiện tượng overfitting.\n",
    "\n",
    "C. Để biến đổi không gian features từ dạng tuyến tính sang dạng phi tuyến.\n",
    "\n",
    "D. Để khắc phục hiện tượng underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KpH7O5fMA_rD"
   },
   "source": [
    "3-C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JE2iRKHqXtRn"
   },
   "source": [
    "4) Nếu mạng neural không áp dụng activation function thì điều gì sẽ xảy ra?\n",
    "\n",
    "A. Các units tại các hidden layers chỉ là biểu diễn tuyến tính của các biến đầu vào.\n",
    "\n",
    "B. Mô hình sẽ có chi phí tính toán cao.\n",
    "\n",
    "C. Cần rất nhiều hidden layers để mô hình có độ chính xác cao.\n",
    "\n",
    "D. Không thể huấn luyện được mô hình."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dW_naJMgBDw_"
   },
   "source": [
    "4-A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WMNPhiPZYmnv"
   },
   "source": [
    "5) Quá trình Feed forward là gì ?\n",
    "\n",
    "A. Tính toán loss function.\n",
    "\n",
    "B. Tính toán đầu ra thông qua dựa vào đầu vào và các kết nối của mạng.\n",
    "\n",
    "C. Tính gradient descent.\n",
    "\n",
    "D. Đếm số lượng tham số của mô hình."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3LYkoplz8HwP"
   },
   "source": [
    "5-B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Nv6DGTKZKLL"
   },
   "source": [
    "6) Mục tiêu của quá trình Back propagation là gì ?\n",
    "\n",
    "A. Tính output của mô hình.\n",
    "\n",
    "B. Tính gradient descent tại từng layers.\n",
    "\n",
    "C. Cập nhật trọng số của mô hình thông qua đạo hàm bậc nhất theo chiều ngược lại từ layer output trở về layer input.\n",
    "\n",
    "D. Tính loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSyz_kFK985g"
   },
   "source": [
    "6-B (**- Sai**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZixC86gZrIY"
   },
   "source": [
    "7) Làm sao để kiểm soát hiện tượng overfitting trong một mạng MLP?\n",
    "\n",
    "A. Thêm thành phần điều chuẩn là các norm chuẩn của trọng số vào loss function.\n",
    "\n",
    "B. Sử dụng dropout để loại bỏ các kết nối của mạng neural một cách ngẫu nhiên.\n",
    "\n",
    "C. Gia tăng số lượng layers.\n",
    "\n",
    "D. A và B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y8ak0nKwaNbI"
   },
   "source": [
    "8) Để tính được phân phối xác suất tại đầu ra của bài toán phân loại đa biến của mạng MLP thì chúng ta sẽ?\n",
    "\n",
    "A. Sử dụng hàm softmax lên các units tại layers cuối cùng.\n",
    "\n",
    "B. Sử dụng hàm ReLU lên các units tại layers cuối cùng.\n",
    "\n",
    "C. Sử dụng hàm tanh lên các units tại layers cuối cùng.\n",
    "\n",
    "D. chỉ cần tổ hợp tuyến tính các units lại layers trước layers cuối cùng.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2H88XI9UatBL"
   },
   "source": [
    "9) Hàm loss function của mạng MLP có dạng như thế nào ?\n",
    "\n",
    "A. Là tổng Cross Entropy của toàn bộ các quan sát.\n",
    "\n",
    "B. Là tổng Cross Entropy của toàn bộ các nhãn.\n",
    "\n",
    "C. Là tổng Cross Entropy của toàn bộ các quan sát và toàn bộ các nhãn.\n",
    "\n",
    "D. A. Là tổng MSE của toàn bộ các quan sát."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qb-jFaXCAkrl"
   },
   "source": [
    "9-C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "janBHU6zbTUs"
   },
   "source": [
    "10) Giả sử sau quá trình Back propagation tính được đạo hàm tại từng trọng số là $D_{ij}^{(l)}$ (là đạo hàm tương ứng với tham số $\\Theta_{ij}^{(l)}$ kết nối unit thứ $i$ của layer $(l-1)$ với unit thứ $j$ của layer $l$). $\\alpha$ là learning rate có giá trị dương và rất nhỏ. Trọng số sẽ được cập nhật như thế nào?\n",
    "\n",
    "A. $\\Theta_{ij}^{(l)} := \\Theta_{ij}^{(l)} + \\alpha D_{ij}^{(l)}$\n",
    "\n",
    "B. $\\Theta_{ij}^{(l)} := \\Theta_{ij}^{(l)} - \\alpha D_{ij}^{(l)}$\n",
    "\n",
    "C. $\\Theta_{ij}^{(l)} := \\Theta_{ij}^{(l)} - \\alpha ||D_{ij}^{(l)}||_1$\n",
    "\n",
    "D. $\\Theta_{ij}^{(l)} := \\Theta_{ij}^{(l)} - \\alpha ||D_{ij}^{(l)}||_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vbRyEi8tDo3s"
   },
   "source": [
    "10 -B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wKdBSdiEVMYB"
   },
   "source": [
    "# II. Thực hành (5 câu, mỗi câu 1 điểm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3z22qDRVHFw"
   },
   "source": [
    "Sử dụng bộ dữ liệu iris về các loài hoa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xTzZLQCBONfQ"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "\n",
    "X = data['data']\n",
    "y = data['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UgwSJamvdomK"
   },
   "source": [
    "Hãy thiết kế một mạng MLP với `input layer (4 units) -->  hidden layer 1 (10 units) --> hidden layer 2 (10 units) --> output layer (3) unit` thông qua việc:\n",
    "\n",
    "\n",
    "1) Xác định kích thước các ma trận trọng số $\\Theta_1, \\Theta_2, \\Theta_3$ tại từng layers. Biết rằng tại các layer có xét đến unit bằng 1 cho _hệ số tự do (bias weight)_. Điền shape vào bên dưới:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NR8oCkfkfElG"
   },
   "outputs": [],
   "source": [
    "# Trả lời:\n",
    "Theta_1.shape = (10,5)\n",
    "Theta_2.shape = (10,11)\n",
    "Theta_3.shape = (3,11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7AY7y6kfDv5"
   },
   "source": [
    "2) Khởi tạo các ma trận trọng số ngẫu nhiên $\\Theta_1$, $\\Theta_2$, $\\Theta_3$ tương ứng với các layers 1, 2, 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iaewA4uawH0P"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Number of units in each layer\n",
    "input_layer_units = 4\n",
    "hidden_layer_1_units = 10\n",
    "hidden_layer_2_units = 10\n",
    "output_layer_units = 3\n",
    "\n",
    "# Initialize the weight matrices using random values from a normal distribution\n",
    "Theta_1 = np.random.randn(hidden_layer_1_units, input_layer_units + 1)\n",
    "Theta_2 = np.random.randn(hidden_layer_2_units, hidden_layer_1_units + 1)\n",
    "Theta_3 = np.random.randn(output_layer_units, hidden_layer_2_units + 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVI2Cum_f4Vo"
   },
   "source": [
    "3) Viết các hàm chức năng gồm:\n",
    "- activation function để biến đổi phi tuyến: Đầu vào là một véc tơ $\\mathbf{z}$.\n",
    "- dense function là hàm tính output tại mỗi layer: Đầu vào là vector $\\mathbf{a}$ (véc tơ gồm các units layer liền trước) và ma trận trọng số $\\mathbf{W}$ kết nối layer liền trước với layer hiện tại. Lưu ý: Cần áp dụng activation function để biến đổi phi tuyến.\n",
    "- softmax function để tính phân phối xác suất: Đầu vào là một véc tơ $\\mathbf{z}$ của các units tại layer cuối cùng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hrvuP4wJwWkc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def dense(a, W):\n",
    "    z = np.dot(W, a)\n",
    "    return sigmoid(z)\n",
    "\n",
    "def softmax(z):\n",
    "    z_exp = np.exp(z)\n",
    "    return z_exp / np.sum(z_exp, axis=0, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3RXIyq9gU6Q"
   },
   "source": [
    "4) Viến hàm thực hiện quá trình feed forward để tính output cho mô hình từ một véc tơ đầu vào. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ruE9oaWMwp6n"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def feedforward(Theta_1, Theta_2, Theta_3, X):\n",
    "    m = X.shape[1]\n",
    "    a1 = np.vstack([np.ones((1, m)), X])\n",
    "    z2 = np.dot(Theta_1, a1)\n",
    "    a2 = np.vstack([np.ones((1, m)), dense(z2, Theta_1)])\n",
    "    z3 = np.dot(Theta_2, a2)\n",
    "    a3 = np.vstack([np.ones((1, m)), dense(z3, Theta_2)])\n",
    "    z4 = np.dot(Theta_3, a3)\n",
    "    a4 = dense(z4, Theta_3)\n",
    "    y_hat = softmax(a4)\n",
    "    return y_hat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0waNs3kyn63q"
   },
   "source": [
    "5) Huấn luyện mô hình trên bộ dữ liệu iris bằng keras theo kiến trúc đã xác định như trên với tỷ lệ phân chia tập train/validation là 80/20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fEZi4Ym-_1z9",
    "outputId": "9624d903-2b3d-4f8c-b9c2-380dff867f98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "4/4 [==============================] - 2s 8ms/step - loss: 1.6000 - accuracy: 0.3583\n",
      "Epoch 2/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 1.4228 - accuracy: 0.3583\n",
      "Epoch 3/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.2771 - accuracy: 0.3583\n",
      "Epoch 4/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.1662 - accuracy: 0.3583\n",
      "Epoch 5/100\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 1.0966 - accuracy: 0.3583\n",
      "Epoch 6/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.0463 - accuracy: 0.3583\n",
      "Epoch 7/100\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 1.0060 - accuracy: 0.3583\n",
      "Epoch 8/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9709 - accuracy: 0.3583\n",
      "Epoch 9/100\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.9363 - accuracy: 0.3583\n",
      "Epoch 10/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.9112 - accuracy: 0.4417\n",
      "Epoch 11/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.8828 - accuracy: 0.6000\n",
      "Epoch 12/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8602 - accuracy: 0.6500\n",
      "Epoch 13/100\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.8378 - accuracy: 0.6583\n",
      "Epoch 14/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8142 - accuracy: 0.6667\n",
      "Epoch 15/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7884 - accuracy: 0.6667\n",
      "Epoch 16/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7667 - accuracy: 0.6833\n",
      "Epoch 17/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7457 - accuracy: 0.7750\n",
      "Epoch 18/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7265 - accuracy: 0.8750\n",
      "Epoch 19/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.7085 - accuracy: 0.9083\n",
      "Epoch 20/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6907 - accuracy: 0.9167\n",
      "Epoch 21/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.6731 - accuracy: 0.9250\n",
      "Epoch 22/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6558 - accuracy: 0.9250\n",
      "Epoch 23/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6390 - accuracy: 0.9333\n",
      "Epoch 24/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6211 - accuracy: 0.9417\n",
      "Epoch 25/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6042 - accuracy: 0.9417\n",
      "Epoch 26/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5871 - accuracy: 0.9333\n",
      "Epoch 27/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5700 - accuracy: 0.9417\n",
      "Epoch 28/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5524 - accuracy: 0.9500\n",
      "Epoch 29/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5353 - accuracy: 0.9500\n",
      "Epoch 30/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.5177 - accuracy: 0.9500\n",
      "Epoch 31/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5008 - accuracy: 0.9500\n",
      "Epoch 32/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.4835 - accuracy: 0.9500\n",
      "Epoch 33/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.4678 - accuracy: 0.9417\n",
      "Epoch 34/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4507 - accuracy: 0.9583\n",
      "Epoch 35/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4350 - accuracy: 0.9583\n",
      "Epoch 36/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4196 - accuracy: 0.9583\n",
      "Epoch 37/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4046 - accuracy: 0.9583\n",
      "Epoch 38/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.3893 - accuracy: 0.9583\n",
      "Epoch 39/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.3753 - accuracy: 0.9583\n",
      "Epoch 40/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.3622 - accuracy: 0.9583\n",
      "Epoch 41/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.3494 - accuracy: 0.9583\n",
      "Epoch 42/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.3368 - accuracy: 0.9583\n",
      "Epoch 43/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.3251 - accuracy: 0.9583\n",
      "Epoch 44/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.3123 - accuracy: 0.9583\n",
      "Epoch 45/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.2992 - accuracy: 0.9583\n",
      "Epoch 46/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.2861 - accuracy: 0.9583\n",
      "Epoch 47/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.2754 - accuracy: 0.9583\n",
      "Epoch 48/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2659 - accuracy: 0.9583\n",
      "Epoch 49/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2561 - accuracy: 0.9583\n",
      "Epoch 50/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2466 - accuracy: 0.9583\n",
      "Epoch 51/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2381 - accuracy: 0.9583\n",
      "Epoch 52/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2315 - accuracy: 0.9583\n",
      "Epoch 53/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2228 - accuracy: 0.9667\n",
      "Epoch 54/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2157 - accuracy: 0.9583\n",
      "Epoch 55/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2099 - accuracy: 0.9583\n",
      "Epoch 56/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2031 - accuracy: 0.9583\n",
      "Epoch 57/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.1979 - accuracy: 0.9667\n",
      "Epoch 58/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1924 - accuracy: 0.9583\n",
      "Epoch 59/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1867 - accuracy: 0.9583\n",
      "Epoch 60/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.1817 - accuracy: 0.9583\n",
      "Epoch 61/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1775 - accuracy: 0.9667\n",
      "Epoch 62/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1726 - accuracy: 0.9583\n",
      "Epoch 63/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1697 - accuracy: 0.9667\n",
      "Epoch 64/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1646 - accuracy: 0.9667\n",
      "Epoch 65/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.1635 - accuracy: 0.9583\n",
      "Epoch 66/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.1579 - accuracy: 0.9667\n",
      "Epoch 67/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.1560 - accuracy: 0.9583\n",
      "Epoch 68/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.1517 - accuracy: 0.9667\n",
      "Epoch 69/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.1482 - accuracy: 0.9667\n",
      "Epoch 70/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1460 - accuracy: 0.9667\n",
      "Epoch 71/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.1426 - accuracy: 0.9667\n",
      "Epoch 72/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1413 - accuracy: 0.9667\n",
      "Epoch 73/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1375 - accuracy: 0.9667\n",
      "Epoch 74/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.1387 - accuracy: 0.9500\n",
      "Epoch 75/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.1328 - accuracy: 0.9667\n",
      "Epoch 76/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1320 - accuracy: 0.9667\n",
      "Epoch 77/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.1296 - accuracy: 0.9667\n",
      "Epoch 78/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1275 - accuracy: 0.9667\n",
      "Epoch 79/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.1258 - accuracy: 0.9667\n",
      "Epoch 80/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1233 - accuracy: 0.9667\n",
      "Epoch 81/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.1215 - accuracy: 0.9750\n",
      "Epoch 82/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1200 - accuracy: 0.9750\n",
      "Epoch 83/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1183 - accuracy: 0.9750\n",
      "Epoch 84/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1166 - accuracy: 0.9833\n",
      "Epoch 85/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1170 - accuracy: 0.9667\n",
      "Epoch 86/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1153 - accuracy: 0.9667\n",
      "Epoch 87/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1132 - accuracy: 0.9667\n",
      "Epoch 88/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1113 - accuracy: 0.9750\n",
      "Epoch 89/100\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.1126 - accuracy: 0.9583\n",
      "Epoch 90/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1095 - accuracy: 0.9833\n",
      "Epoch 91/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.1082 - accuracy: 0.9750\n",
      "Epoch 92/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1071 - accuracy: 0.9750\n",
      "Epoch 93/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1058 - accuracy: 0.9750\n",
      "Epoch 94/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1064 - accuracy: 0.9750\n",
      "Epoch 95/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1036 - accuracy: 0.9833\n",
      "Epoch 96/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1032 - accuracy: 0.9750\n",
      "Epoch 97/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1019 - accuracy: 0.9667\n",
      "Epoch 98/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.1009 - accuracy: 0.9750\n",
      "Epoch 99/100\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.0998 - accuracy: 0.9750\n",
      "Epoch 100/100\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0992 - accuracy: 0.9833\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.1032 - accuracy: 1.0000\n",
      "Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# One-hot encode the target labels\n",
    "y = to_categorical(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Initializing the model\n",
    "model = Sequential()\n",
    "\n",
    "# Adding the input layer with 4 units and activation function 'relu'\n",
    "model.add(Dense(10, input_dim=4, activation='relu'))\n",
    "\n",
    "# Adding the first hidden layer with 10 units and activation function 'relu'\n",
    "model.add(Dense(10, activation='relu'))\n",
    "\n",
    "# Adding the second hidden layer with 10 units and activation function 'relu'\n",
    "model.add(Dense(10, activation='relu'))\n",
    "\n",
    "# Adding the output layer with 3 units and activation function 'softmax'\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', test_acc)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
