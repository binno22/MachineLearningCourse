{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SR4mkBllVIIw"
   },
   "source": [
    "# I. Lý thuyết (10 câu, 0.5 điểm/câu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0nTCKfnmVP_6"
   },
   "source": [
    "1) Phát biểu nào sau đây là đúng về kiến trúc MLP\n",
    "\n",
    "A. Là kiến trúc gồm nhiều Layers liên tiếp xử lý dữ liệu theo thứ tự.\n",
    "\n",
    "B. Mỗi một layer của MLP sẽ bao gồm nhiều units, mỗi unit đóng vai trò tương tự như một biến.\n",
    "\n",
    "C. Thứ tự các layers của MLP là `input layer --> các hidden layers --> output layer`\n",
    "\n",
    "(D). Cả ba đáp án trên."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A42FkbHNWC6J"
   },
   "source": [
    "2) Các đặc trưng được tạo ra từ MLP có gì khác biệt so với các thuật toán machine learning truyền thống ?\n",
    "\n",
    "A. Là những đặc trưng được xác định từ trước thông qua feature engineering từ người xây dựng mô hình.\n",
    "\n",
    "B. Là những đặc trưng được tạo thành từ Polynormial Feature.\n",
    "\n",
    "C. Là những đặc trưng đã được chuẩn hóa theo Min-max scaling.\n",
    "\n",
    "(D). Là những đặc trưng ẩn (_latent features_) được học thông qua quá trình huấn luyện mô hình."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AxRBoeqtWBky"
   },
   "source": [
    "3) Vì sao khi tính toán một unit tại layer tiếp theo thì cần áp dụng activation function lên tổ hợp tuyến tính của các units đầu vào từ layer ngay liền trước?\n",
    "\n",
    "A. Để giảm thiểu chi phí tính toán.\n",
    "\n",
    "(B.) Để khắc phục hiện tượng overfitting. **-Sai**\n",
    "\n",
    "C. Để biến đổi không gian features từ dạng tuyến tính sang dạng phi tuyến.\n",
    "\n",
    "D. Để khắc phục hiện tượng underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JE2iRKHqXtRn"
   },
   "source": [
    "\n",
    "\n",
    "```\n",
    "# Выбран кодовый формат\n",
    "```\n",
    "\n",
    "4) Nếu mạng neural không áp dụng activation function thì điều gì sẽ xảy ra?\n",
    "\n",
    "A. Các units tại các hidden layers chỉ là biểu diễn tuyến tính của các biến đầu vào.\n",
    "\n",
    "B. Mô hình sẽ có chi phí tính toán cao.\n",
    "\n",
    "(C). Cần rất nhiều hidden layers để mô hình có độ chính xác cao. **- Sai**\n",
    "\n",
    "D. Không thể huấn luyện được mô hình."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WMNPhiPZYmnv"
   },
   "source": [
    "5) Quá trình Feed forward là gì ?\n",
    "\n",
    "A. Tính toán loss function.\n",
    "\n",
    "(B). Tính toán đầu ra thông qua dựa vào đầu vào và các kết nối của mạng.\n",
    "\n",
    "C. Tính gradient descent.\n",
    "\n",
    "D. Đếm số lượng tham số của mô hình."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Nv6DGTKZKLL"
   },
   "source": [
    "6) Mục tiêu của quá trình Back propagation là gì ?\n",
    "\n",
    "A. Tính output của mô hình.\n",
    "\n",
    "B. Tính gradient descent tại từng layers.\n",
    "\n",
    "(C). Cập nhật trọng số của mô hình thông qua đạo hàm bậc nhất theo chiều ngược lại từ layer output trở về layer input.\n",
    "\n",
    "D. Tính loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZixC86gZrIY"
   },
   "source": [
    "7) Làm sao để kiểm soát hiện tượng overfitting trong một mạng MLP?\n",
    "\n",
    "A. Thêm thành phần điều chuẩn là các norm chuẩn của trọng số vào loss function.\n",
    "\n",
    "B. Sử dụng dropout để loại bỏ các kết nối của mạng neural một cách ngẫu nhiên.\n",
    "\n",
    "C. Gia tăng số lượng layers.\n",
    "\n",
    "(D). A và B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y8ak0nKwaNbI"
   },
   "source": [
    "8) Để tính được phân phối xác suất tại đầu ra của bài toán phân loại đa biến của mạng MLP thì chúng ta sẽ?\n",
    "\n",
    "A. Sử dụng hàm softmax lên các units tại layers cuối cùng.\n",
    "\n",
    "B. Sử dụng hàm ReLU lên các units tại layers cuối cùng.\n",
    "\n",
    "C. Sử dụng hàm tanh lên các units tại layers cuối cùng.\n",
    "\n",
    "(D). chỉ cần tổ hợp tuyến tính các units lại layers trước layers cuối cùng. **- Sai**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2H88XI9UatBL"
   },
   "source": [
    "9) Hàm loss function của mạng MLP có dạng như thế nào ?\n",
    "\n",
    "A. Là tổng Cross Entropy của toàn bộ các quan sát.\n",
    "\n",
    "B. Là tổng Cross Entropy của toàn bộ các nhãn.\n",
    "\n",
    "(C). Là tổng Cross Entropy của toàn bộ các quan sát và toàn bộ các nhãn.\n",
    "\n",
    "D. A. Là tổng MSE của toàn bộ các quan sát."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "janBHU6zbTUs"
   },
   "source": [
    "10) Giả sử sau quá trình Back propagation tính được đạo hàm tại từng trọng số là $D_{ij}^{(l)}$ (là đạo hàm tương ứng với tham số $\\Theta_{ij}^{(l)}$ kết nối unit thứ $i$ của layer $(l-1)$ với unit thứ $j$ của layer $l$). $\\alpha$ là learning rate có giá trị dương và rất nhỏ. Trọng số sẽ được cập nhật như thế nào?\n",
    "\n",
    "A. $\\Theta_{ij}^{(l)} := \\Theta_{ij}^{(l)} + \\alpha D_{ij}^{(l)}$\n",
    "\n",
    "(B). $\\Theta_{ij}^{(l)} := \\Theta_{ij}^{(l)} - \\alpha D_{ij}^{(l)}$\n",
    "\n",
    "C. $\\Theta_{ij}^{(l)} := \\Theta_{ij}^{(l)} - \\alpha ||D_{ij}^{(l)}||_1$\n",
    "\n",
    "D. $\\Theta_{ij}^{(l)} := \\Theta_{ij}^{(l)} - \\alpha ||D_{ij}^{(l)}||_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wKdBSdiEVMYB"
   },
   "source": [
    "# II. Thực hành (5 câu, mỗi câu 1 điểm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3z22qDRVHFw"
   },
   "source": [
    "Sử dụng bộ dữ liệu iris về các loài hoa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xTzZLQCBONfQ"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "\n",
    "X = data['data']\n",
    "y = data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JDtnQARksg9M",
    "outputId": "5e8db521-510a-40da-94c3-26d1a24b4b75"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UgwSJamvdomK"
   },
   "source": [
    "Hãy thiết kế một mạng MLP với `input layer (4 units) -->  hidden layer 1 (10 units) --> hidden layer 2 (10 units) --> output layer (3) unit` thông qua việc:\n",
    "\n",
    "\n",
    "1) Xác định kích thước các ma trận trọng số $\\Theta_1, \\Theta_2, \\Theta_3$ tại từng layers. Biết rằng tại các layer có xét đến unit bằng 1 cho _hệ số tự do (bias weight)_. Điền shape vào bên dưới:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NR8oCkfkfElG"
   },
   "outputs": [],
   "source": [
    "# Trả lời:\n",
    "'''\n",
    "có j = 1 2 3 4 \n",
    " s_j = 4 10 10 3\n",
    "\n",
    "col_theta= s_j +1\n",
    "row_theta= s_(j+1) * (s_j +1)\n",
    "'''\n",
    "Theta_1.shape =(10, 5)\n",
    "Theta_2.shape =(10,11)\n",
    "Theta_3.shape =(3, 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7AY7y6kfDv5"
   },
   "source": [
    "2) Khởi tạo các ma trận trọng số ngẫu nhiên $\\Theta_1$, $\\Theta_2$, $\\Theta_3$ tương ứng với các layers 1, 2, 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iI_SkQ6A1UOf",
    "outputId": "8e0d1e96-2cdc-4be3-bd55-742bae10f05e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                50        \n",
      "                                                                 \n",
      " activation (Activation)     (None, 10)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 10)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 33        \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 3)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 193\n",
      "Trainable params: 193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Activation\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "inpt = Input(shape = (4))\n",
    "hidden_1 = Dense(10)(inpt)\n",
    "act1 = Activation('relu')(hidden_1)\n",
    "hidden_2 = Dense(10)(act1)\n",
    "act2 = Activation('relu')(hidden_2)\n",
    "hidden_3 = Dense(3)(act2)\n",
    "output = Activation('softmax')(hidden_3)\n",
    "model = Model(inputs = [inpt], outputs = [output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVI2Cum_f4Vo"
   },
   "source": [
    "3) Viết các hàm chức năng gồm:\n",
    "- activation function để biến đổi phi tuyến: Đầu vào là một véc tơ $\\mathbf{z}$.\n",
    "- dense function là hàm tính output tại mỗi layer: Đầu vào là vector $\\mathbf{a}$ (véc tơ gồm các units layer liền trước) và ma trận trọng số $\\mathbf{W}$ kết nối layer liền trước với layer hiện tại. Lưu ý: Cần áp dụng activation function để biến đổi phi tuyến.\n",
    "- softmax function để tính phân phối xác suất: Đầu vào là một véc tơ $\\mathbf{z}$ của các units tại layer cuối cùng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RZGsD9Cv8Bzi"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def act_function(z):\n",
    "  return 1/(1 + np.exp(-z))\n",
    "\n",
    "def dense_funtion(a,W):\n",
    "  return act_function(a.dot(W,T))\n",
    "  \n",
    "def softmax(z):\n",
    "  e_x = np.exp(z - np.max(z))\n",
    "  return e_x / e_x.sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3RXIyq9gU6Q"
   },
   "source": [
    "4) Viết hàm thực hiện quá trình feed forward để tính output cho mô hình từ một véc tơ đầu vào.[ ](https://)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YRCTWbNS8BPd"
   },
   "outputs": [],
   "source": [
    "def feed_forward(x):\n",
    "  layer_1= dense_funtion(x)\n",
    "  layer_2= dense_funtion(layer_1)\n",
    "  layer_3= dense_funtion(layer_2)\n",
    "  return layer_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0waNs3kyn63q"
   },
   "source": [
    "5) Huấn luyện mô hình trên bộ dữ liệu iris bằng keras theo kiến trúc đã xác định như trên với tỷ lệ phân chia tập train/validation là 80/20."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
