{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"12.SVM-SoftMargin-Practice.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO726QevJXsmJ3bh6X9Z5bh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"xV17dmJw6o94"},"source":["# Soft Margin Classification\n","\n","In Scikit-Learn’s SVM classes, you can control this balance using the C hyperparameter: a smaller C value leads to a wider margine but more margin violations. \n","\n","![](https://phamdinhkhanh.github.io/deepai-book/_images/SVM_23_0.png)\n","\n","Above figure shows the decision boundaries and margins of two soft margin SVM classifiers on a nonlinearly separable dataset. On the left, using a high C value the classifier makes fewer margin violations but ends up with a smaller margin. On the right, using a low C value the margin is much larger, but many instances end up on the margin. However, it seems likely that the second classifier will generalize better.\n","\n","\n","The following Scikit-Learn code loads the iris dataset, scales the features, and then trains a linear SVM model (using the LinearSVC class with C = 0.1 and the hinge loss function, described shortly) to detect Iris-Virginica flowers."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qemtR1aj1cQm","executionInfo":{"status":"ok","timestamp":1638109544537,"user_tz":-420,"elapsed":785,"user":{"displayName":"khanhblog AI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNCi9Qnch9sWXSuvX4N5yijAGEjX1IvfmN-95m=s64","userId":"06481533334230032014"}},"outputId":"d5a5f730-677b-480e-a611-b44785c3dd6e"},"source":["import numpy as np\n","from sklearn import datasets\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.svm import LinearSVC\n","iris = datasets.load_iris()\n","X = iris[\"data\"][:, (2, 3)] # petal length, petal width\n","y = (iris[\"target\"] == 2).astype(np.float64) # Iris-Virginica\n","svm_clf = Pipeline((\n","  (\"scaler\", StandardScaler()),\n","  (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\")),\n","))\n","svm_clf.fit(X, y)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Pipeline(steps=[('scaler', StandardScaler()),\n","                ('linear_svc', LinearSVC(C=1, loss='hinge'))])"]},"metadata":{},"execution_count":1}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xmsfaj8516Yc","executionInfo":{"status":"ok","timestamp":1638109544541,"user_tz":-420,"elapsed":19,"user":{"displayName":"khanhblog AI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNCi9Qnch9sWXSuvX4N5yijAGEjX1IvfmN-95m=s64","userId":"06481533334230032014"}},"outputId":"ab135278-0f96-40ff-899b-e0c2d8f84f18"},"source":["svm_clf.predict([[5.7,  6.5]])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1.])"]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"z_IHV-BM1tno"},"source":["Alternatively, you could use the SVC class, using SVC(kernel=\"linear\", C=1), but it\n","is much slower, especially with large training sets, so it is not recommended. Another\n","option is to use the SGDClassifier class, with SGDClassifier(loss=\"hinge\",\n","alpha=1/(m*C)). This applies regular Stochastic Gradient Descent to\n","train a linear SVM classifier. It does not converge as fast as the LinearSVC class, but it\n","can be useful to handle huge datasets that do not fit in memory (out-of-core train‐\n","ing), or to handle online classification tasks."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1zXFhgd92Cdi","executionInfo":{"status":"ok","timestamp":1638109788628,"user_tz":-420,"elapsed":357,"user":{"displayName":"khanhblog AI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNCi9Qnch9sWXSuvX4N5yijAGEjX1IvfmN-95m=s64","userId":"06481533334230032014"}},"outputId":"c213c32f-0069-452a-c914-154fd670cafa"},"source":["import numpy as np\n","from sklearn import datasets\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import SGDClassifier\n","\n","iris = datasets.load_iris()\n","X = iris[\"data\"][:, (2, 3)] # petal length, petal width\n","y = (iris[\"target\"] == 2).astype(np.float64) # Iris-Virginica\n","m = 2\n","C = 1\n","sgd_clf = Pipeline((\n","  (\"scaler\", StandardScaler()),\n","  (\"sgd_svm\", SGDClassifier(alpha=1/(m*C), loss=\"hinge\")),\n","))\n","\n","sgd_clf.fit(X, y)\n","sgd_clf.predict([[5.7,  6.5]])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1.])"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"q4rEwly_2DAh"},"source":["# Non-Linear classifier SVM\n","\n","Although linear SVM classifiers are efficient and work surprisingly well in many\n","cases, many datasets are not even close to being linearly separable. One approach to\n","handling nonlinear datasets is to add more features, such as polynomial features; in some cases this can result in a linearly separable dataset."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mQYUuYfV2EV4","executionInfo":{"status":"ok","timestamp":1638110282082,"user_tz":-420,"elapsed":414,"user":{"displayName":"khanhblog AI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNCi9Qnch9sWXSuvX4N5yijAGEjX1IvfmN-95m=s64","userId":"06481533334230032014"}},"outputId":"9063bb8a-a711-449e-a099-e9559da2f75e"},"source":["from sklearn.datasets import make_moons\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import PolynomialFeatures\n","polynomial_svm_clf = Pipeline((\n","  (\"poly_features\", PolynomialFeatures(degree=3)),\n","  (\"scaler\", StandardScaler()),\n","  (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\"))\n","))\n","polynomial_svm_clf.fit(X, y)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Pipeline(steps=[('poly_features', PolynomialFeatures(degree=3)),\n","                ('scaler', StandardScaler()),\n","                ('svm_clf', LinearSVC(C=10, loss='hinge'))])"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"UwfdwP5-243_"},"source":["# Polynomial Kernel\n","\n","Adding polynomial features is simple to implement and can work great with all sorts\n","of Machine Learning algorithms (not just SVMs), but at a low polynomial degree it\n","cannot deal with very complex datasets, and with a high polynomial degree it creates\n","a huge number of features, making the model too slow.\n","Fortunately, when using SVMs you can apply an almost miraculous mathematical\n","technique called the kernel trick (it is explained in a moment). It makes it possible to get the same result as if you added many polynomial features, even with very high-degree polynomials, without actually having to add them. So there is no combinatorial explosion of the number of features since you don’t actually add any features. This trick is implemented by the SVC class. Let’s test it on the moons dataset:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ISlbuyfb3NED","executionInfo":{"status":"ok","timestamp":1638110669507,"user_tz":-420,"elapsed":443,"user":{"displayName":"khanhblog AI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNCi9Qnch9sWXSuvX4N5yijAGEjX1IvfmN-95m=s64","userId":"06481533334230032014"}},"outputId":"7811f0a9-c5e9-40a3-8396-c5bc88094b14"},"source":["from sklearn.svm import SVC\n","poly_kernel_svm_clf = Pipeline((\n","  (\"scaler\", StandardScaler()),\n","  (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n","))\n","poly_kernel_svm_clf.fit(X, y)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Pipeline(steps=[('scaler', StandardScaler()),\n","                ('svm_clf', SVC(C=5, coef0=1, kernel='poly'))])"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"Bsm9GQdC3308"},"source":["# Gaussian RBF kernel\n","\n","Just like the polynomial features method, the similarity features method can be useful\n","with any Machine Learning algorithm, but it may be computationally expensive to\n","compute all the additional features, especially on large training sets. However, once\n","again the kernel trick does its SVM magic: it makes it possible to obtain a similar\n","result as if you had added many similarity features, without actually having to add\n","them. Let’s try the Gaussian RBF kernel using the SVC class:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GULIusCZ34AM","executionInfo":{"status":"ok","timestamp":1638074920261,"user_tz":-420,"elapsed":282,"user":{"displayName":"khanhblog AI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNCi9Qnch9sWXSuvX4N5yijAGEjX1IvfmN-95m=s64","userId":"06481533334230032014"}},"outputId":"00eabc8b-152d-4f6c-98f4-5a0919f74d70"},"source":["rbf_kernel_svm_clf = Pipeline((\n","  (\"scaler\", StandardScaler()),\n","  (\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=0.001))\n","))\n","rbf_kernel_svm_clf.fit(X, y)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Pipeline(steps=[('scaler', StandardScaler()),\n","                ('svm_clf', SVC(C=0.001, gamma=5))])"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"UmmgE97h5YYo"},"source":["We can select kernel in list `linear, poly, rbf, sigmoid` "]},{"cell_type":"markdown","metadata":{"id":"AuIh9mg25Ev9"},"source":["# Time complexity\n","\n","$m$ is number of features and $n$ is number of instances. The time complexity of three SVM algorithms as below:\n","\n","![](https://imgur.com/6Ypa544.png)"]}]}